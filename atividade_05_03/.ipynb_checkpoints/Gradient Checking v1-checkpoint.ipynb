{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verificação de Gradiente\n",
    "\n",
    "Bem vindo a mais uma tarefa do curso de Aprendizado em Profundidade!! Nesta tarefa você irá aprender a implementar e utilizar a verificação de gradiente.  \n",
    "\n",
    "Você é parte de uma equipe reponsável por desenvolver um sistema de pagamento através de celular disponível mundialmente, e foi solicitado que você desenvolvesse um modelo de aprendizado em profundidade para detectar fraudes toda vez que fosse feito um pagamento. Você deve verificar se este pagamento foi fraudulento, por exemplo, verificando se algum hacker entrou na conta daquele cliente.  \n",
    "\n",
    "Acontece que a propagação para trás é desafiadora para ser implementada e, muitas vezes, apresenta problemas. Como esta é uma missão crítica para a aplicação o CEO da sua empresa quer ter certeza que a implementação da propagação para trás está correta. Para provar que o sistema está funcionando você vai utilizar a verificação de gradiente.\n",
    "\n",
    "Vamos a ela!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from testCases import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Como funciona a verificação de gradiente?\n",
    "\n",
    "A propagação para trás computa os valores dos gradientes $\\frac{\\partial J}{\\partial \\theta}$, onde $\\theta$ indica os parâmetros do modelo. $J$ é determinado utilizando a propagação para frente e a função de perda. \n",
    "\n",
    "Como a propagação para frente é reativamente simples de ser implementada, você tem certeza de que ela está correta e você está confidente de que ela está determinando o valor de $J$ corretamente. Portanto, você pode utilizar o seu código para determinar $J$ e verificar a computação de $\\frac{\\partial J}{\\partial \\theta}$. \n",
    "\n",
    "Vamos olhar novamente a definição de derivada (ou gradiente):\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Se você não está familiarizado com a notação \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\", isto é apenas uma forma de dizer que  \"$\\varepsilon$ é muito muito pequena.\"\n",
    "\n",
    "Nós sabemos que:\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ é o que você quer verificar. \n",
    "- Você pode determinar $J(\\theta + \\varepsilon)$ e $J(\\theta - \\varepsilon)$ (no caso de $\\theta$ ser um número real), pois você tem certeza de que a implementação de $J$ está correta. \n",
    "\n",
    "Vamos utilizar a equação (1) e um valor pequeno para $\\varepsilon$ para convencer o CEO de que o código está computando $\\frac{\\partial J}{\\partial \\theta}$ corretamente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Verificação de gradiente em 1 dimensão\n",
    "\n",
    "Considere  uma função linear em 1D $J(\\theta) = \\theta x$. O modelo contém apenas uma única variável real como parâmetro $\\theta$, e recebe $x$ como entrada.\n",
    "\n",
    "Você deve implementar o código para determinar $J(.)$ e sua derivada $\\frac{\\partial J}{\\partial \\theta}$. Você irá então utilizar a verificação de gradiente para ter certeza que sua implementação da derivada para $J$ está correta.  \n",
    "\n",
    "<img src=\"images/1Dgrad_kiank.png\" style=\"width:600px;height:250px;\">\n",
    "<caption><center> <u> **Figura 1** </u>: **modelo linear em 1D**<br> </center></caption>\n",
    "\n",
    "O diagrama acima mostra as principais etapas de computação: Primeiro comece com $x$, e determine o valor da função $J(x)$ (\"propagação para frente\"). Compute então a derivada $\\frac{\\partial J}{\\partial \\theta}$ (\"propagação para trás\"). \n",
    "\n",
    "**Exercício**: implemente a \"propagação para frente\" e \"propagação para trás\" para esta função simples. Isto é , compute os dois $J(.)$ (\"propagação para frente\") e sua derivada com relação a $\\theta$ (\"propagação para trás\"), em duas funções separadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: propagação para frente\n",
    "\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para frente (compute J) apresentado na Figura 1 (J(theta) = theta * x)\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- uma entrada com valor real\n",
    "    theta -- nosso parâmetro, também um número real\n",
    "    \n",
    "    Retorna:\n",
    "    J -- o valor da função J, determinado utilizando a formula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    \n",
    "    ### COMECE SEU CÓDIGO AQUI ### (approx. 1 linha)\n",
    "    \n",
    "    ### TERMINE O CÓDIGO AQUI ###\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table style=>\n",
    "    <tr>\n",
    "        <td>  ** J **  </td>\n",
    "        <td> 8</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Agora, implemente a propagação para trás (computação da derivada) da Figura 1. Isto é, compute a derivada de $J(\\theta) = \\theta x$ com relação a $\\theta$. Para auxiliar neste cálculo, você deve obter $dtheta = \\frac { \\partial J }{ \\partial \\theta} = x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: PROPAGAÇÃO PARA TRÁS\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Computa a derivada de J com relação a theta (Veja a Figura 1).\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- uma entrada de valor real\n",
    "    theta -- nosso parâmetro, também um número real\n",
    "    \n",
    "    Retorna:\n",
    "    dtheta -- o gradiente do custo com relação a theta\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (approx. 1 linha)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  ** dtheta **  </td>\n",
    "        <td> 2 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Para mostrar que a função `backward_propagation()` está computando corretamente a derivada $\\frac{\\partial J}{\\partial \\theta}$, vamos implementar a verificação de gradiente.\n",
    "\n",
    "**Instruções**:\n",
    "- Primeiro determine \"gradapprox\" usando a fórmula acima (1) e um valor pequeno para $\\varepsilon$. Aqui estão as etapas a serem seguidas: \n",
    "    1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "    2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "    3. $J^{+} = J(\\theta^{+})$\n",
    "    4. $J^{-} = J(\\theta^{-})$\n",
    "    5. $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "- Depois, determine o valor do gradiente usando a função de propagação para trás e armazene o resultado na variável \"grad\"\n",
    "- Finalmente, compute a diferença relativa entre \"gradapprox\" e \"grad\" usando a seguinte fórmula:\n",
    "$$ diferença = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "Você irá precisar de 3 etapas para computar esta fórmula:\n",
    "   - 1'. Determine o numerador utilizando np.linalg.norm(...)\n",
    "   - 2'. Determine o denominador. Você precisa chamar a função np.linalg.norm(...) duas vezes.\n",
    "   - 3'. Execute a divisão.\n",
    "- Se a diferença for pequena (da ordem de $10^{-7}$), você pode ficar confidente de que sua implementação da propagação para trás está correta. Caso contrário, pode existir algum erro na computação da derivada. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: verificação de gradiente\n",
    "\n",
    "def gradient_check(x, theta, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para trás apresentada na Figura 1.\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- uma entrada de valor real\n",
    "    theta -- nosso parâmetro, também um valor real\n",
    "    epsilon -- pequeno valor utilizado para aproximar o gradiente com a fórmula (1)\n",
    "    \n",
    "    Retorna:\n",
    "    difference -- diferença entre (2) o gradiente aproximado e o gradiente determinado na propagação para trás.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute gradapprox using o lado esquerdo da formula (1). epsilon é pequeno o bastante, você não precisa se preocupar\n",
    "    # sobre o limite.\n",
    "    ### INÍCIO DO CÓDIGO AQUI ### (approx. 5 linhas)\n",
    "                                   # Step 1\n",
    "                                   # Step 2\n",
    "                                   # Step 3\n",
    "                                   # Step 4\n",
    "                                   # Step 5\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    # Verifique se gradapprox é próximo o bastante da saída da propagação para trás\n",
    "    ### INÍCIO DO CÓDIGO AQUI ###  (approx. 1 linha)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    ### INÍCIO DO CÓDIGO AQUI ###  (approx. 3 linhas)\n",
    "                                 # Step 1'\n",
    "                                 # Step 2'\n",
    "                                 # Step 3'\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print (\"O gradiente está correto!\")\n",
    "    else:\n",
    "        print (\"O gradiente está errado!\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"diferença = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "O gradiente está correto!\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  ** diferença **  </td>\n",
    "        <td> 2.9193358103083e-10 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, a diferença é menor que o limite de $10^{-7}$. Você tem motivos para estar confidente em sua implementação da propagação para trás. \n",
    "\n",
    "Agora, num caso mais geral, a sua função de custo $J$ possui mais que um entrada em 1D. Quando você está treinando uma rede neural, $\\theta$ é um vetor formado pro diversas matrizes $W^{[l]}$ e valores de bias $b^{[l]}$! É importante saber implementar a verificação de gradiente para entradas de grnades dimensões. Vamos implementar este caso! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Verificação do gradiente para funções N-dimensionais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A figura abaixo descreve a propagação para frente e para trás do seu modelo de detecção de fraudes.\n",
    "\n",
    "<img src=\"images/NDgrad_kiank.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **Figura 2** </u>: **rede neural profunda**<br>*LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID*</center></caption>\n",
    "\n",
    "Vamos verificar a implementação das propagações para frente e para trás. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implementa a propagação para frente (e determina o valor do custo) apresentado na Figure 3.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- conjunto de treinamento com m exemplos\n",
    "    Y -- valores de saída para os m exemplos \n",
    "    parâmetros -- dicionário em python contendo os parâmetros \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- matriz de pesos no formato (5, 4)\n",
    "                    b1 -- vetor de bias no formato (5, 1)\n",
    "                    W2 -- matriz de pesos no formato (3, 5)\n",
    "                    b2 -- vetor de bias no formato (3, 1)\n",
    "                    W3 -- matriz de pesos no formato (1, 3)\n",
    "                    b3 -- vetor de bias no formato (1, 1)\n",
    "    \n",
    "    Retorna:\n",
    "    cost -- a função de custo (custo logístico para um exemplo)\n",
    "    \"\"\"\n",
    "    \n",
    "    # recupere os parâmetros\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Custo\n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, execute a propagação para trás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implementa a propagação para trás apresentada na figure 2.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- entrada, no formato (tamanho_da_entrada, 1)\n",
    "    Y -- saída correta\n",
    "    cache -- cache das saídas obtidas na propagação para frente\n",
    "    \n",
    "    Retorna:\n",
    "    gradientes -- Um dicionário com os gradientesdo custo com relação a cada parâmetro, variáveis de ativação e pre-ativação.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) * 2\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Você conseguiu alguns resultados de detecção de fraude do seu conjunto de teste, mas você não está totalmente convencido sobre o seu modelo. Ninguém é perfeito! Vamos implementar a verificação de gradiente para conferir se os gradientes determinados estão corretos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como a verificação de gradiente funciona?**.\n",
    "\n",
    "Da mesma forma que em 1) e 2), você quer comparar \"gradapprox\" com o valor do gradiente computado pela propagação para trás. A fórmula ainda é:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "Porém, $\\theta$ não é mais um escalar. É um dicionário chamado de \" parâmetros\". Nós implementamos uma função \"`dictionary_to_vector()`\" para você you. Ela converte os \"parameters\" do dicionário em um vetor chamado de \"values\", obtido pela reformatação de todos os parâmetros (W1, b1, W2, b2, W3, b3) em vetores e concatenando os vetores.\n",
    "\n",
    "A função inversa é \"`vector_to_dictionary`\"  que retorna o dicionário de parâmetros.\n",
    "\n",
    "<img src=\"images/dictionary_to_vector.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> **Figura 3** </u>: **dictionary_to_vector() e vector_to_dictionary()**<br> Você irá utilizar estas funções para fazer a verificação do gradiente</center></caption>\n",
    "\n",
    "Nós ainda convertemos o dicionário de gradientes em um vetor \"grad\" usando gradients_to_vector(). Você não precisa se preocupar com isto.\n",
    "\n",
    "**Exercício**: Implemente gradient_check_n().\n",
    "\n",
    "**Instruções**: Aqui está o pseudo-código que irá ajudá-lo a implementar a verificação do gradiente.\n",
    "\n",
    "Para cada i em num_parameters:\n",
    "- Para determinar `J_plus[i]`:\n",
    "    1. Faça $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Faça $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calcule $J^{+}_i$ usando a `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- Para determinar `J_minus[i]`: faça a mesma coisa usando $\\theta^{-}$\n",
    "- Determine $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Você determinou o vetor gradapprox, onde gradapprox[i] é uma aproximação do gradiente com relação ao parâmetro na posição \"i\". Agora é possível comparar o vetor gradapprox com os valores dos gradientes dados pela função de propagação para trás. Da mesma forma que fizemos no caso em 1D (Etapas 1', 2', 3'), compute: \n",
    "$$ diferença = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Verifica se a propagação para trás computa corretamente o gradiente da função de custo dada pela propagação para frente.\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionário python que contém os parâmetros \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- saída da função backward_propagation_n, contém os gradientes do custo com relação aos parâmetros. \n",
    "    x -- entrada, no formato (tamanho_da_entrada, 1)\n",
    "    y -- saída correta\n",
    "    epsilon -- valor pequeno para determinar o gradiente aproximado pela fórmula(1)\n",
    "    \n",
    "    Retorna:\n",
    "    difference -- diferença (2) entre o gradiente aproximado e o gradiente determinado pela propagação para trás.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Organizando as variáveis\n",
    "    parameters_values, _ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Computando gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Entradas: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" é utilizado porque a função para o cálculo de J possui duas saídas mas nós estamos interessados \n",
    "        # apenas na primeira\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (approx. 3 linhas)\n",
    "                                              # Step 1\n",
    "                                              # Step 2\n",
    "                                              # Step 3\n",
    "        ### TÉRMINO D CÓDIGO ###\n",
    "        \n",
    "        # Compute J_minus[i]. Entrada: \"parameters_values, epsilon\". Saída = \"J_minus[i]\".\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (approx. 3 linhas)\n",
    "                                             # Step 1\n",
    "                                             # Step 2        \n",
    "                                             # Step 3\n",
    "        ###  TÉRMINO D CÓDIGO ###\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (approx. 1 linha)\n",
    "        \n",
    "        ###  TÉRMINO D CÓDIGO ###\n",
    "    \n",
    "    # Compare gradapprox com a propagação para trás determinando a diferença.\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (approx. 1 linha)\n",
    "                                              # Step 1'\n",
    "                                              # Step 2'\n",
    "                                              # Step 3'\n",
    "    ### TÉRMINO D CÓDIGO ###\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"Existe um problema com a diferença na propagação para trás = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"A propagação para trás está correta!!! diferença = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperado**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  ** Existe um problema com a diferença na propagação para trás = **  </td>\n",
    "        <td> 0.285093156781 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que existe um problema com a função `backward_propagation_n` fornecida! Ainda bem que você fez a verificação de gradiente. Volte para a função `backward_propagation` e tente encontrar/corrigir os erros *(Dica: verifique dW2 e db1)*. Execute novamente a verificação de gradiente quando você acar que resolveu os problemas. Lembre-se de executar novamente a célula que define `backward_propagation_n()` se você modificar o código. \n",
    "\n",
    "Você consegue fazer as correções necessárias no código? Mesmo não sendo avaliado sugere-se que você procure arrumar o código e rodar novamente o verificador de gradientes. \n",
    "\n",
    "**Nota** \n",
    "- Verificação de gradiente é um processo lento! Aproximar o gradiente com $\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ é caro computacionalmente. Por esta razão nós não executamos esta verificação em cada interação durante o treinamento. Apenas algumas vezes para ter certeza se a propagação para trás está correta. \n",
    "- A verificação de gradiente, como apresentado, não funciona com dropout. Você deve executar a verificação de gradiente sem o dropout para verificar a propagação para trás e depois adicionar o dropout.  \n",
    "\n",
    "Parabéns, você pode ter certeza de que o seu modelo para detectar fraudes está funcionando corretamente.  \n",
    "\n",
    "<font color='blue'>\n",
    "**O que você deve lembrar deste notebook**:\n",
    "- Verificação de gradiente determina o quanto os gradientes da propagação para trás estão próximos daaproximação numérica dos gradientes.\n",
    "- Verificação de gradiente é lenta, portanto, não deve ser utilizada em todas asa interações durante o treinamento. Utilize apenas em algumas interações para verificar os valores e, em seguida, deslique o uso da verificação. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "n6NBD",
   "launcher_item_id": "yfOsE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
