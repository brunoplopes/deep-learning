{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural Convolucional: Passo a Passo\n",
    "\n",
    "Bem vindo a primeira tarefa do curso 3! Netsa tarefa você irá implementar as camadas convolucionais (CONV) e pooling (POOL) em numpy, incluindo a propagação para frente e a programação para trás (opcional).  \n",
    "\n",
    "**Notação**:\n",
    "- Sobrescrito $[l]$ indica um objeto da $l^{ésima}$ camada. \n",
    "    - Exemplo: $a^{[4]}$ é a ativação da $4^{a}$ camada. $W^{[5]}$ e $b^{[5]}$ são os parâmetros da $5^{a}$ camada.\n",
    "\n",
    "\n",
    "- Sobrescrito $(i)$ indica um objeto do $i^{ésimo}$ exemplo. \n",
    "    - Exemplo: $x^{(5)}$ é o $5^{o}$ exemplo de treinamento.\n",
    "    \n",
    "    \n",
    "- Subscrito $i$ indica a $i^{ésima}$ entrada de um vetor.\n",
    "    - Exemplo: $a^{[l]}_3$ indica a $a^{a}$ entrada da camada de ativação $l$, assumindo que esta é uma camada totalmente conectada (FC).\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$ e $n_C$ indicam respectivamente a altura, largur e número de canais de uma dada camada. Se você quer se referenciar a uma camada específica $l$, você pode escrever $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ e $n_{C_{prev}}$ indicam respectivamente a altura, largura e número de canais de uma camada anterior. Para inidicar uma camada específica $l$, você pode escrever $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. \n",
    "\n",
    "Estamos assumindo que você já está acostumado com `numpy`. Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Pacotes\n",
    "\n",
    "Vamos importar os pacotes necessários para esta tarefa. \n",
    "- [numpy](www.numpy.org) é o principal pacote para computação científica do Python.\n",
    "- [matplotlib](http://matplotlib.org) é uma biblioteca para plotar gráficos em Python.\n",
    "- np.random.seed(1) é utilizada para manter as chamadas de funções aleatórias consistentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # ajusta o padrão para os tamanhos de figuras\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Descrição da tarefa\n",
    "\n",
    "Voc6e irá implementar os blocos, um a um, de uma rede neural convolucional! Cada função que você implementar terá instruções detalhadas para auxiliá-lo nos passos necessários para a implementação:\n",
    "\n",
    "- Funções da convolução, incluem:\n",
    "    - Padding com zeros.\n",
    "    - Kernel de convolução. \n",
    "    - Convolução para frente.\n",
    "    - Convolução para trás (opcional).\n",
    "- Funções de Pooling, incluindo:\n",
    "    - Pooling para frente.\n",
    "    - Criar uma máscara. \n",
    "    - Distribuir valores.\n",
    "    - Pooling para trás (opcional).\n",
    "    \n",
    "Este notebook irá guiá-lo para implementar estas funções em `numpy` do zero. Na próxima tarefa você irá utilizar as funções equivalentes do tensorFlow para construir o modelo abaixo:\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**Nota** para cada função para frente existe uma equivalente para trás. Portanto, em cada etapado seu modelo para frente você deve armazenar parâmetros em uma cache. Estes parâmetros serão utilizados para determinar os gradientes na propagação para trás.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Rede Neural Convolucional\n",
    "\n",
    "Embora os frameworks de programação tornem fácil o uso de convoluções, ele continuam sendo um dos conceitos mais dificeis de se entender em Aprendizado Profundo. Uma camada de convolução transforma um volume de entrada em um volme de saída de tamanho diferente, conforme mostrado abaixo: \n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "Nesta parte, você irá construir cada um dos processos de uma camada de convolução. Primeiro você irá implementar duas funç/òes de auxilio: uma para fazer o padding usando zeros e a outra para computar a convolução propriamente.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Padding com zeros\n",
    "\n",
    "O processo de padding com zeros em volta da borda de uma imagem:\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Padding com zeros**<br> Imagem (3 canais, RGB) com um padding de tamanho 2 (p=2). </center></caption>\n",
    "\n",
    "Os principais beneficios do processo de padding são:\n",
    "\n",
    "- Ele permite que se utlize uma camada CONV sem que se reduza o tamanho da saída (altura e largura do volume). Isto é importante para o caso de uso de redes mais profundas, caso não se utilize o padding a altura e largura podem ficar muito pequenas. Um caso especial importante é a convolução \"same\", onde a altura e largura na saída são exatamente iguais as da entrada.  \n",
    "\n",
    "- Ela auxilia na manutenção das informações da borda da imagem, sem o padding, poucos valores nas camadas seguintes seriam afetados por pixels nas bordas da imagem.\n",
    "\n",
    "**Exercício**: Implemente a seguinte função que adiciona zeros em todas as imagens de um batch X de exemplos. [Use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). Note que, se você deseja fazer um pad em um array \"a\" no formato $(5,5,5,5,5)$ com `p = 1` para a $2^{a}$ dimensão, `p = 3` para a $4^{a}$ dimensão e `p = 0` para o restante, voce deve fazer a seguinte chamada:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Faça um pad de zeros em todas as imagens de uma base de dados X. O padding deve ser aplicado na altura e largura das imagens\n",
    "    conforme ilustrado na figura 1. \n",
    "    \n",
    "    Argumentos:\n",
    "    X -- um array numpy no formato (m, n_H, n_W, n_C) representando um batch de m imagens\n",
    "    pad -- valor inteiro, número de linhas/colunas a ser adicionadas com zeros em cada imagem.\n",
    "        \n",
    "    Returns:\n",
    "    X_pad -- m imagens do batch no formato (m, n_H+pad, n_W+pad, n_C)\n",
    "    \"\"\"\n",
    "    ### INICIE SEU CÓDIGO AQUI ### (≈ 1 linha)\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad), (pad,pad),(0,0)), 'constant', constant_values=(0,0)) \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formato de x = (4, 3, 3, 2)\n",
      "formato de x com padding = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24ef369be10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"formato de x =\", x.shape)\n",
    "print (\"formato de x com padding =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **formato de x**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 3, 3, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **formato de x com padding**:\n",
    "        </td>\n",
    "        <td>\n",
    "           (4, 7, 7, 2)\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "        </td>\n",
    "    </tr>\n",
    "        <tr>\n",
    "        <td>\n",
    "            **x_pad[1,1]**:\n",
    "        </td>\n",
    "        <td>\n",
    "           [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Uma etapa da convolução \n",
    "\n",
    "Nesta parte você irá implementar uma etapa do processo de convolução. Você aplica o filtro em uma posição da imagem de entrada. Esta função será utilizada para construir a unidade de convolução que:  \n",
    "\n",
    "- Recebe como entrada um volume \n",
    "- Aplica o filtro em cada posição da entrada\n",
    "- Retorna como saída um novo volume (normalmente de tamanho diferente)\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 2** </u><font color='purple'>  : **Operação de Convolução**<br> com um filtro de tamanho 3x3 e um **stride** de 1 (stride = quanto o filtro é movido para cada operação) </center></caption>\n",
    "\n",
    "Em uma aplicação de visão computacional, cada valor na matriz à esuqerda corresponde ao valor de um pixel da imagem. Fazemos a convolução com um filtro 3x3 multiplicando seus valores elemento a elemento com os valores da imagem e então somamos o resultado das multiplicações e adicionando um bias. Nesta primeira etapado exercício você irá implementar um etapa simples do processo de convolução que corresponde a aplicação do filtro em uma única posição da imagem e obtendo como saída um valor real. \n",
    "\n",
    "Mais tarde utilizaremos esta função para fazer o processo de convolução em toda a imagem de entrada. \n",
    "\n",
    "**Exercício**: Implemente conv_single_step(). [Dica](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Aplica um filtro definido pelos parâmetros W sobre uma parte da imagem de entrada (a_slice_prev) da saída da camada de \n",
    "    ativacao anterior.\n",
    "        \n",
    "    Argumentos:\n",
    "    a_slice_prev -- parte dos dados de entrado no formato (f, f, n_C_prev)\n",
    "    W -- parâmetros do filtro a ser utilizado -matriz no formato (f, f, n_C_prev)\n",
    "    b -- parâmetro de Bias - possui formato (1, 1, 1)\n",
    "    \n",
    "    Retorna:\n",
    "    Z -- um valor escalar, resultado da convolução pelo filtro W, e adiciona o bias bno final\n",
    "    \"\"\"\n",
    "\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 3 linhas de código)\n",
    "    # produto elemento a elemento entre a parte da imagem de entrada e o filtro.\n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "    # Soma dos valores dos produtos feitos acima.\n",
    "    Z = np.sum(s)\n",
    "    # Adiciona o Bias b em Z. Defina b como um float() de forma que Z seja um valor escalar.\n",
    "    Z = Z+np.float(b)\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -6.99908945068\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.3 - Redes Neurais Convolucionais - Passo para frente\n",
    "\n",
    "No passo para frente você irá pegar vários filtros e fazer a convolução deles com a entrada. Cada convolução retorna uma matriz em 2D de saída. Você irá empilhar estas matrizes para obter um volume em 3D:  \n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**Exercício**: Implemente a função abaixo para fazer a convolução dos filtros W sobre a entrada da ativação  A_prev. Esta função recebe como entrada A_prev, a saída da ativação da camada anterior (para um batch de m exemplos), F filtros/pesos indicados por W, e um vetor de bias indicado como b, onde cada filtro possui seu próprio bias. Por fim, você ainda tem acesso ao dicionário de hiper-parâmentros contendo o valor de stride e padding.  \n",
    "\n",
    "**Dica**: \n",
    "1. Para selecionar uma fatia 2x2 do canto superior esquerdo da matriz \"a_prev\" (formato (5,5,3)), você deve fazer:\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "Isto será útil para você definir `a_slice_prev` abaixo, utilizando os índices `inicio/fim` que você definir.\n",
    "2. Para definir a_slice você primeiro precisa definir os cantos `vert_inicio`, `vert_fim`, `horiz_inicio` e `horiz_fim`. A figura abaixo deve ajudá-lo a encontrar cada um dos cantos definidos usando h, w, f e s no código abaixo.\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **Definição de uma fatia utilizando inicio/fim horizontal e vertical (com um filtro 2x2)** <br> A figura mostra apenas um canal.  </center></caption>\n",
    "\n",
    "\n",
    "**Lembre-se**:\n",
    "As fórmulas relacionadas com o formato da saída da convolução são:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{número de filtros utilizados na convolução}$$\n",
    "\n",
    "Para este exercício não iremos nos preocupar com vetorização e implementaremos tudo usando loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implementa a etapa de propagação para frente para a função de convolução.\n",
    "    \n",
    "    Argumentos:\n",
    "    A_prev -- saída da camada de ativação anterior, um array numpy no formato (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Pesos, array numpy no formato (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, array numpy no formato (1, 1, 1, n_C)\n",
    "    hparameters -- dicionário python contendo os valores de \"stride\" e \"pad\"\n",
    "        \n",
    "    Retorna:\n",
    "    Z -- saída da convolução, um array numpy no formato (m, n_H, n_W, n_C)\n",
    "    cache -- cache dos valores necessários para a propagação para trás.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    # recupere as dimensões de A_prev (≈1 linha)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)\n",
    "    \n",
    "    # recupere as dimensões de W (≈1 linha)\n",
    "    (f, f, n_C_prev, n_C) = np.shape(W)\n",
    "    \n",
    "    # recupere as informações de stride e pad de \"hparameters\" (≈2 linhas)\n",
    "    stride = hparameters.get(\"stride\")\n",
    "    pad = hparameters.get(\"pad\")\n",
    "    \n",
    "    # Compute a dimensão do volume de saida CONV utilizando a fórmula dada acima. Dica: use int() para floor. (≈2 linhas)\n",
    "    n_H = int((n_H_prev-f+2*pad)/stride)+1\n",
    "    n_W = int((n_W_prev-f+2*pad)/stride)+1\n",
    "    \n",
    "    # Inicialize o volume de saída Z com zeros. (≈1 linha)\n",
    "    Z = np.zeros((m,n_H,n_W,n_C))\n",
    "    \n",
    "    # Crie A_prev_pad fazendo o padding de A_prev. (~1 linha)\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m):                               # Faça o loop sobre todos os exemplos de treinamento\n",
    "        a_prev_pad = A_prev_pad[i]                   # Selecione o i-ésimo exemplo já com o padding\n",
    "        for h in range(0,n_H):                       # Faça um loop sobre o eixo vertical do volume de saída\n",
    "            for w in range(0,n_W):                   # Faça um loop sobre o eixo horizontal do volume de saída\n",
    "                for c in range(n_C):                 # Faça um loop sobre os canais (= #filtros) do volume de saída\n",
    "                    \n",
    "                    # Encontre os cantos da fatia atual (≈4 linhas)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Use os cantos para definir a fatia em 3D do a_prev_pad (Veja a dica na célula acima). (≈1 linha)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Faça a convolução da fatia em 3D com o filtro W e adicione o bias b \n",
    "                    # para obter a saída do neurônio. (≈1 linha)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:,:,:,c],b[:,:,:,c])\n",
    "                                        \n",
    "    ### TÉRMINO DO CÓDIGO AQUI ###\n",
    "    \n",
    "    # Verificando o formato da saída para saber se está correto.\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Salva a informação na cache para a propagação para trás\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.048995203528855794\n",
      "Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
      "  5.18531798  8.75898442]\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **média dos Zs**\n",
    "        </td>\n",
    "        <td>\n",
    "            0.0489952035289\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z[3,2,1]**\n",
    "        </td>\n",
    "        <td>\n",
    "            [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
    "  5.18531798  8.75898442]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **cache_conv[0][1][2][3]**\n",
    "        </td>\n",
    "        <td>\n",
    "            [-0.20075807  0.18656139  0.41005165]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, a camada CONV deve ter uma função de ativação, portanto iremos adicionar a seguinte linha de código: \n",
    "\n",
    "```python\n",
    "# Convolução da janela para obter a saída do neurônio\n",
    "Z[i, h, w, c] = ...\n",
    "# Aplique a ativação\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "\n",
    "Você não precisa fazer isto aqui. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Camada de Pooling \n",
    "\n",
    "A camada de pooling (POOL) reduz as dimensões de altura e largura da entrada. Ele auxilia na redução da computação e também torna os detectores de características mais invariantes de sua posição na entrada. Os dois tipos de pooling são:  \n",
    "\n",
    "- Camada de Max-pooling: utiliza uma janela de tamanho ($f, f$) sobre a entrada e armazena o maior valor encontrado na janela.\n",
    "\n",
    "- Camada Average-pooling: utiliza uma janela de tamanho ($f, f$) sobre a entrada e armazena a média dos valores encontrados na janela. \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/ave_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "As camadas de pooling não possuem parâmetros a serem treinados pelo processo de propagação para trás. Porém, elas possuem hiper-parâmetros como o tamanho da janela$f$. Este valor especifica a altura e largura da  janela  utilizada para determinar o máximo ou a média.   \n",
    "\n",
    "### 4.1 - Pooling para frente\n",
    "Agora você irá implementar as duas funções: MAX-POOL e AVG-POOL, na mesma função. \n",
    "\n",
    "**Exercício**: Implemente o passo para frente da camada de pooling. Siga as dicas abaixo: \n",
    "\n",
    "**Dicas**:\n",
    "Como não existe padding, as fórmulas que definem o formato da saída do pooling são: \n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implementa o passo para frente da camada de pooling.\n",
    "    \n",
    "    Argumentos:\n",
    "    A_prev -- dados de entrada, um array numpy no formato (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- dicionário python contendo \"f\" e \"stride\"\n",
    "    mode -- o modo de pooling desejado, definido como uma string (\"max\" ou \"average\")\n",
    "    \n",
    "    Retorna:\n",
    "    A -- saída da camada de pooling, um array numpy array no formato (m, n_H, n_W, n_C)\n",
    "    cache -- cache utilizada na passo de propagação para trás da camada de pooling, contém a entrada e hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Recupera as dimensões do formato da entrada\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Recupera os hiper-parâmetros de \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define as dimensões da saída\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Inicializa a matriz de saída\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### INICIE O SEU CÓDIGO AQUI ###\n",
    "    for i in range(m):                           # loop sobre os exemplos de treinamento\n",
    "        for h in range(n_H):                     # loop no eixo vertical do volume de saída\n",
    "            for w in range(n_W):                 # loop no eixo horizontal do volume de saída\n",
    "                for c in range (n_C):            # loop sobre os canais do volume de saída\n",
    "                    \n",
    "                    # Determine os cantos da fatia atual (≈4 linhas)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Use os cantos para definir a fatia no i-ésimo exemplo de treinamento de A_prev, no canal c. (≈1 linha)\n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    # Compute a operação de pooling desejada na fatia. Use um comando if para diferenciar os modos. \n",
    "                    # Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.average(a_prev_slice)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    # Armazene a entrada e os hparameters na \"cache\" para fazer a propagação para trás.\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Verifica se as dimensões da saída estão corretas.\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modo = max\n",
      "A = [[[[1.74481176 0.86540763 1.13376944]]]\n",
      "\n",
      "\n",
      " [[[1.13162939 1.51981682 2.18557541]]]]\n",
      "\n",
      "modo = média\n",
      "A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"modo = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"modo = média\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada:**\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>\n",
    "    A  =\n",
    "    </td>\n",
    "        <td>\n",
    "         [[[[ 1.74481176  0.86540763  1.13376944]]]\n",
    "\n",
    "\n",
    " [[[ 1.13162939  1.51981682  2.18557541]]]]\n",
    "\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    A  =\n",
    "    </td>\n",
    "        <td>\n",
    "         [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
    "\n",
    "\n",
    " [[[-0.22154621  0.51716526  0.48155844]]]]\n",
    "\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns! Você implementou os passos da propagação para frente de uma rede convolucional. \n",
    "\n",
    "O restante deste notebook é opcional e não será avaliado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Propagação para trás em uma rede convolucional (Opcional)\n",
    "\n",
    "Nos frameworks mais novos para aprendizado profundo você deve apenas implementar o passo para frente, logo, a maioria dos profissionais que atuam em aprendizado profundo não se preocupam com o passo para trás. O passo para trás nas redes convolucionais é complicado. Porém, se você desejar, você pode trabalhar nesta parte opcional da tarefa que irá te dar uma idéia de como é feita a propagação para trás em uma rede convolucional.  \n",
    "\n",
    "Já foi visto como se implementa o passo para trás em uma rede totalmente conectada, o passo para trás é utilizado para computar as derivadas com relação ao custo para atualizar os parâmetros. Da mesma forma, nas redes convolucionais você pode calcular as derivadas com relação ao custo para a tualizar os parâmetros da rede. As equações da propagação para trás não são triviais e nós não trabalhamos com elas na aula, mas elas serão apresentadas abaixo. \n",
    "\n",
    "### 5.1 - Camada convolucional - passo para trás \n",
    "\n",
    "Vamos começar implementando o passo para trás de uma camada CONV. \n",
    "\n",
    "#### 5.1.1 - Computando dA:\n",
    "Esta é a fórmula para se determinar $dA$ com relação ao custo para um certo filtro $W_c$ e um dado exemplo de treinamento:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "Onde $W_c$ é um filtro e $dZ_{hw}$ é um escalar que corresponde ao gradiente do custo com relação a saída da camada CONV Z na h-ésima linha e w-ésima coluna (correspondendo ao produto escalar (dot product) feito no i-ésimo stride para a esquerda e j-ésimo stride para baixo). Note que a cada vez, multiplicamos o mesmo filtro $W_c$ por um valor diferente de dZ quando atualizamos o valor de dA. Fazemos isto porque, quando computamos a propagação para frente cada filtro faz o produto e soma para uma fatia diferente. Logo, quando computamos a propagação para trás para dA, estamos simplesmente adicionando os gradientes de todas as fatias.  \n",
    "\n",
    "Em código, dentro do loop apropriado, esta fórmula resolve isto:\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - Computando dW:\n",
    "Esta é a fórmula para computar $dW_c$ ($dW_c$ é a derivda de um filtro) com relação ao custo:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Onde $a_{slice}$ corresponde a fatia que foi utilizada para gerar a ativação $Z_{ij}$. Logo, isto acaba dando o gradiente para $W$ com relação aquela fatia. Como ele está relacionado ao mesmo $W$, nós simplesmente adicionamos todos os gradientes para obter $dW$. \n",
    "\n",
    "Em código, dentro do loop apropriado, esta fórmula resolve isto:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - Computando db:\n",
    "\n",
    "Esta fórmula determina $db$ com relação ao custo para um determinado filtro $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "Como já foi visto anteriormente nas redes neurais básicas, $db$ é determinado somando-se $dZ$. Neste caso você irá somar sobre todos os gradientes da saida da CONV (Z) com relação ao custo. \n",
    "\n",
    "Em código, dentro do loop apropriado, esta fórmula resolve isto:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**Exercício**: Implemente a função `conv_backward` abaixo. Você deveria somar sobre todos os exemplos de treinamentos, filtros, alturas e larguras. Na sequência você deve computar as derivadas usando as equações 1, 2 e 3 acima.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para trás para a função de convolução\n",
    "    \n",
    "    Argumentos:\n",
    "    dZ -- gradiente do custo com relação a saída da camada CONV (Z), array numpy no formato (m, n_H, n_W, n_C)\n",
    "    cache -- cache dos valores necessários para a função conv_backward(), saídas da conv_forward()\n",
    "    \n",
    "    Retorna:\n",
    "    dA_prev -- gradiente do custo com relação a entrada da camada CONV (A_prev),\n",
    "               array numpy no formato (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradiente do custo com relação aos pesos da camada CONV (W)\n",
    "          array numpy no formato (f, f, n_C_prev, n_C)\n",
    "    db -- gradiente do custo com relação ao bias da camada CONV (b)\n",
    "          array numpy no formato (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    # Recupere a informação da \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Recupere as dimensões de A_prev\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Recupere as dimensões de W\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Recupere as informações de \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Recupere as informações de dZ\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Inicialize dA_prev, dW, db com os formatos correspondentes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "\n",
    "    # Faça o padding em A_prev e dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop sobre os exemplos de treinamento\n",
    "        \n",
    "        # selecione o i-ésimo exemplo de treinamento de A_prev_pad e dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop sobre o eixo vertical do volume de saída\n",
    "            for w in range(n_W):               # loop sobre o eixo horizontal do volume de saída\n",
    "                for c in range(n_C):           # loop sobre os canais do volume de saída\n",
    "                    \n",
    "                    # Encontre os cantos da fatia atual\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Use os cantos para definir a fatia de a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Atualize os gradientes para a janela e os parâmetros do filtro utilizando as fórmulas acima. \n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Defina o i-ésimo exemplo de treinamento dA_prev para da_prev_pad sem o padding (Dica: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad]\n",
    "    ### TÉRMINO DO CÓDIGO AQUI ###\n",
    "    \n",
    "    # verificando se o formato da saída está correto\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"média dA =\", np.mean(dA))\n",
    "print(\"média dW =\", np.mean(dW))\n",
    "print(\"média db =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Saída esperada: **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **média dA**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.45243777754\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **média dW**\n",
    "        </td>\n",
    "        <td>\n",
    "            1.72699145831\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **média db**\n",
    "        </td>\n",
    "        <td>\n",
    "            7.83923256462\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 camada de Pooling - passo para trás\n",
    "\n",
    "Na sequência, vamos implementar o passo para trás da camada de pooling, começando com a camada MAX-POOL. Embora a camada de pooling não tenha parâmetros a serem atualizados na propagação para trás, você ainda deve passar os gradientes pela camada de pooling para poder determinar as atualizações das camadas anteriores ao pooling.  \n",
    "\n",
    "### 5.2.1 Max pooling - passo para trás  \n",
    "\n",
    "Antes de olhar a camada de pooling, vamos criar uma função auxiliar chamada  `create_mask_from_window()` que faça o seguinte: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "Como você pode ver, esta função cria uma máscara que indica onde o valor máximo esta na matriz, Verdade (1) indica a posição do máximo em X e as outras entradas são Falsas (0). Voc6e verá depois que o passo para trás para o \"average\"pooling será similar a este, porém, utilizando uma máscara diferente.   \n",
    "\n",
    "**Exercício**: Implemente `create_mask_from_window()`. Esta função será útil para a propagação para trás do pooling.  \n",
    "Dicas:\n",
    "- [np.max()]() pode ser útil. Ele computa o valor máximo em um array.\n",
    "- Se você tem uma matriz X e um escalar x: `A = (X == x)` irá retornar A, do mesmo tamanho de X de forma que: \n",
    "```\n",
    "A[i,j] = Verdade se X[i,j] = x\n",
    "A[i,j] = Falso se X[i,j] != x\n",
    "```\n",
    "- Aqui, você não precisa se preocupar se o máximo ocorrer em várias posições da matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Cria uma máscara de uma matriz de x, para identificar a posição do máximo em x.\n",
    "    \n",
    "    Argumentos:\n",
    "    x -- Array no formato (f, f)\n",
    "    \n",
    "    Retorna:\n",
    "    mask -- Array no mesmo formato de x, contendo Verdade na posição do máximo valor em x e Falso nas demais posições\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE O CÓDIGO AQUI ### (≈1 linha)\n",
    "    mask = (x==x.max())\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6243453636632417\n",
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "máscara =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"máscara = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Saída esperada:** \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**x =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "[[ 1.62434536 -0.61175641 -0.52817175] <br>\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**máscara =**\n",
    "</td>\n",
    "<td>\n",
    "[[ True False False] <br>\n",
    " [False False False]]\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porque precisamos saber a posição do máximo? O valor do máximo é o valor que influenciou a saída e portanto o custo. A propagação para trás está computando os gradientes com relação ao custo, logo, qualquer fator que influencie no custo deve ter um gradiente diferente de zero. Logo, a propagação para trás, irá propagar o gradiente de volta para este valor em particular que influenciou o custo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Average pooling - passo para trás \n",
    "\n",
    "No max pooling, para cada janela de entrada, a influência vinha apenas de um valor, o máximo. No average pooling todos os elementos contribuem de forma igual para a saída. Para implementar a propagação para trás iremos também implementar uma função auxiliar que reflita isto. \n",
    "\n",
    "Por exemplo, se o average pooling foi feito com um filtro 2x2, então, a máscara que utilizaremos para a propagação para trás ficará assim:  \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "Isto implica que, cada posição na matriz $dZ$ contribuiu igualmente na saída, porque no passo para frente utilizamos a média.  \n",
    "\n",
    "**Exercício**: Implemente a função abaixo que distribui igualmente o valor de dz em uma matriz de dimensão 'shape'. [Dica](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distribui os valores de entrada em uma matriz de dimensão \n",
    "    \n",
    "    Argumentos:\n",
    "    dz -- um escalar de entrada\n",
    "    shape -- as dimensões (n_H, n_W) da matriz de saída para o qual se deseja distribuir o valor de dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array no formato (n_H, n_W) onde o valor de dz está distribuido uniformemente.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE O CÓDIGO AQUI ###\n",
    "    # Recupere as dimensões de shape (≈1 linha)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute o valor para distribuir na matriz (≈1 linha)\n",
    "    average = dz/(n_H*n_W)\n",
    "    \n",
    "    # Crie uma matriz onde cada entrada possui o valor de 'average' (≈1 linha)\n",
    "    a = average*np.ones((n_H,n_W))\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valor dsitribuído = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('valor dsitribuído =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "valor distribuído =\n",
    "</td>\n",
    "<td>\n",
    "[[ 0.5  0.5]\n",
    "<br\\> \n",
    "[ 0.5  0.5]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Colocando tudo junto: Pooling para trás \n",
    "\n",
    "Agora nós já temos tudo o que precisamos para computar a propagação para trás em uma camada de pooling.\n",
    "\n",
    "**Exercício**: Implemente a função `pool_backward` nos dois modos (`\"max\"` e `\"average\"`). Voce irá utilizar novamente 4 loops (interando sobre os exemplos de treinamento, altura, largura e canais). Voce deve utilizar um comando `if/elif` para vrificar se o modo é do tipo `'max'` ou `'average'`. Se for igual a '`average`' você deve chamar a função `distribute_value()` implementada acima para criar uma matriz no mesmo formato de `a_slice`. Caso contrário, se o modo for '`max`', chame a função `create_mask_from_window()` e multiplique ela pelo valor de dZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implemente o passo para trás da camada de pooling\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente do custo com relação à saída da camada de pooling, tem o mesmo formato de A.\n",
    "    cache -- saída cache do passo para frente da camada de pooling, contém as entradas e os hparameters da camada. \n",
    "    mode -- o modo de pooling mode que foi utilizado, definido como uma string (\"max\" ou \"average\")\n",
    "    \n",
    "    Retorna:\n",
    "    dA_prev -- gradiente de custo com relação a entrada da camada de pooling, tem o mesmo formato de A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE SEU CÓDIGO AQUI ###\n",
    "    \n",
    "    # Recupere a informação da cache (≈1 linha)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Recupere os hiper-parâmetros de \"hparameters\" (≈2 linhas)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Recupere as dimensões de do formato de A_prev e do formato de dA (≈2 linhas)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Inicialize dA_prev com zeros (≈1 linha)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop sobre todos o exemplos de treinamento\n",
    "        \n",
    "        # selecione o exemplo de treinamento de A_prev (≈1 linha)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop sobre o eixo vertical\n",
    "            for w in range(n_W):               # loop sobre o eixo horizontal\n",
    "                for c in range(n_C):           # loop sobre os canais\n",
    "                    \n",
    "                    # Determine os cantos da fatia atual (≈4 linhas)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Compute a propagação para trás para ambos os modos.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use os cantos e \"c\" para definir a fatia atual de a_prev (≈1 linha)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        # Crie a máscara da fatia a_prev_slice (≈1 linha)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Defina dA_prev como sendo dA_prev + (a máscara multiplicada pelo valor de dA atual) (≈1 linha)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask*dA[i,h,w,c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Obtenha o valor de dz a partir de dA (≈1 linha)\n",
    "                        dz = dA[i,h,w,c]\n",
    "                        # Defina o formato do filtro como fxf (≈1 linha)\n",
    "                        shape = (f,f)\n",
    "                        # Distribua o valor de dz na fatia atual de dA_prev, isto é, some os valores distribuidos de dz. (≈1 linha)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(dz,shape)\n",
    "                        \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    # Verificando se o formato da saída está correto\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74481176421648\n",
      "-0.2493703754774101\n",
      "1.462107937044974\n",
      "-0.2493703754774101\n",
      "1.74481176421648\n",
      "-0.2493703754774101\n",
      "1.462107937044974\n",
      "-0.2493703754774101\n",
      "1.1337694423354374\n",
      "1.1447237098396141\n",
      "1.1337694423354374\n",
      "1.1447237098396141\n",
      "0.9008559492644118\n",
      "1.1447237098396141\n",
      "0.9015907205927955\n",
      "1.1447237098396141\n",
      "1.6598021771098705\n",
      "0.7420441605773356\n",
      "1.6598021771098705\n",
      "0.7420441605773356\n",
      "1.6598021771098705\n",
      "1.6924546010277466\n",
      "1.6598021771098705\n",
      "2.100255136478842\n",
      "0.3001703199558275\n",
      "1.6924546010277466\n",
      "0.3001703199558275\n",
      "2.100255136478842\n",
      "0.8389834138745049\n",
      "0.9311020813035573\n",
      "0.8389834138745049\n",
      "0.9311020813035573\n",
      "2.1855754065331614\n",
      "1.5198168164221988\n",
      "2.1855754065331614\n",
      "-0.07557171302105573\n",
      "2.1855754065331614\n",
      "1.5198168164221988\n",
      "2.1855754065331614\n",
      "0.8279746426072462\n",
      "0.31563494724160523\n",
      "0.8761689211162249\n",
      "0.31563494724160523\n",
      "0.8279746426072462\n",
      "0.2300947353643834\n",
      "0.7620111803120247\n",
      "0.1865613909882843\n",
      "1.1294839079119197\n",
      "1.198917879901507\n",
      "0.6980320340722189\n",
      "0.4234943540641129\n",
      "1.2245077048054989\n",
      "0.4034916417908\n",
      "0.6980320340722189\n",
      "0.7405564510962748\n",
      "1.2245077048054989\n",
      "0.4034916417908\n",
      "0.593578523237067\n",
      "0.8461606475850334\n",
      "0.31515939204229176\n",
      "0.35054597866410736\n",
      "0.31515939204229176\n",
      "1.121417708235664\n",
      "0.4089005379368278\n",
      "1.6276507531489064\n",
      "1.9671017492547347\n",
      "1.2737559301587766\n",
      "1.9671017492547347\n",
      "1.6276507531489064\n",
      "0.8633453175440216\n",
      "0.7928068659193477\n",
      "0.8633453175440216\n",
      "0.8018610318713447\n",
      "0.5505374959762154\n",
      "0.8688861570058679\n",
      "0.7504116398650081\n",
      "0.8018610318713447\n",
      "0.6183802619985245\n",
      "0.8688861570058679\n",
      "0.7504116398650081\n",
      "modo = max\n",
      "média de dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "média de dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"modo = max\")\n",
    "print('média de dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('média de dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "modo = max:\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**média de dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.          0.        ] <br>\n",
    " [ 5.05844394 -1.68282702] <br>\n",
    " [ 0.          0.        ]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "modo = average\n",
    "<table> \n",
    "<tr> \n",
    "<td>\n",
    "\n",
    "**média de dA =**\n",
    "</td>\n",
    "\n",
    "<td>\n",
    "\n",
    "0.145713902729\n",
    "\n",
    "  </td>\n",
    "</tr>\n",
    "\n",
    "<tr> \n",
    "<td>\n",
    "**dA_prev[1,1] =** \n",
    "</td>\n",
    "<td>\n",
    "[[ 0.08485462  0.2787552 ] <br>\n",
    " [ 1.26461098 -0.25749373] <br>\n",
    " [ 1.17975636 -0.53624893]]\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parabéns !\n",
    "\n",
    "Você completou mais uma tarefa. Agora você consegue entender como uma rede convolucional funciona. Você implementou todos os blocos de uma rede neural convolucional. Na próxima tarefa você irá implementar uma ConvNet utilizando TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
