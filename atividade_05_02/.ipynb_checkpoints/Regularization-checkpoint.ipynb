{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularização\n",
    "\n",
    "Bem-vindo a segunda tarefa desta semana. Modelos de aprendizado profundo possuem muita flexibilidade e capacidade, porém, **overfitting** pode ser um problema sério se o conjunto de treinamento não for grande o bastante. A rede pode aprender bem no conjunto de treinamento mas ela **não generaliza** para exemplos que ela nunca viu! \n",
    "\n",
    "Nesta tarefa você irá aprender a utilizar regularização em seus modelos de aprendizado profundo. \n",
    "\n",
    "Vamos primeiro importar os pacotes necessários para esta tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pacotes importantes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_fsf_dataset, predict_dec\n",
    "from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "from testCases import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # define o tamanho padrão dos gráficos.\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Definição do Problema**: Você acabou de ser contratado como um especialista em IA pela Federação Francesa de Futebol. Eles querem que você recomende posições onde o goleiro da equipe francesa deva chutar a bola de forma que os jogadores do time frances possam dominá-la.  \n",
    "\n",
    "<img src=\"images/field_kiank.png\" style=\"width:600px;height:350px;\">\n",
    "<caption><center> <u> **Figura 1** </u>: **Campo de Futebol**<br> O goleiro chuta a bola no ar, os jogadores de cada equipe lutam para dominar a bola </center></caption>\n",
    "\n",
    "\n",
    "A FFF forneceu a você a seguinte base de dados dos últimos 10 jogos da França. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X, train_Y, test_X, test_Y = load_fsf_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada ponto corresponde a uma posição no campo de futebol onde a bola foi dominada após o chute do goleiro da França estando defendendo o goldo lado esquerdo do campo. \n",
    "- Um ponto azul quer dizer que o time da França dominou a bola após o chute do goleiro.\n",
    "- Um ponto vermelho indica que a bola foi dominada pelo time adversário.\n",
    "\n",
    "**Seu objetivo**: Utilizar aprendizado profundo para encontrar as posições no campo onde o goleiro deveria chutar a bola. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analise dos dados**: Esta base de dados é um pouco ruidosa, porém, parece que encontrar uma linha diagonal separando a metade superior esquerda do campo (pontos azuis) da parte inferior direita (pontos vermelhos) deve ser o suficiente. \n",
    "\n",
    "Você irá primeiro tentar um modelo sem utilizar regularização. Em seguida você irá aprender a aplicar regularização e decidir qual o modelo melhor para o problema da FFF. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Modelo não regularizado\n",
    "\n",
    "Você irá utilizar uma rede neural já implementada para você. Este modelo pode ser utilizado: \n",
    "- no *regularization mode* -- ajustando o valor de `lambd` para um valor diferente de zero. Usamos \"`lambd`\" no lugar de \"`lambda`\" porque \"`lambda`\" é uma palavra reservada em Python. \n",
    "- no *dropout mode* -- ajustando o valor de `keep_prob` para um valor menor que 1.\n",
    "\n",
    "Primeiro vamos tentar um modelo sem regularização. Em seguida você irá implementar:\n",
    "- *Regularização L2* -- com as funções: \"`compute_cost_with_regularization()`\" e \"`backward_propagation_with_regularization()`\"\n",
    "- *Dropout* -- com as funções: \"`forward_propagation_with_dropout()`\" e \"`backward_propagation_with_dropout()`\"\n",
    "\n",
    "Em cada parte, você executará este modelo com as entradas corretas de forma que ele chame as funções que você implementou. Verifique o código abaixo para se familiarizar com o modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    Implementa uma rede neural com 3 camadas: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- dados de entrada, no formato (tamanho da entrada, número de exemplos)\n",
    "    Y -- vetor com valores corretos da saída (1 para azul/0 para vermelho), no formato (tamanho saída, número de exemplos)\n",
    "    learning_rate -- taxa de aprendizado da otimização.\n",
    "    num_iterations -- número de interações do loop de otimização.\n",
    "    print_cost -- Se True, imprime o valor da função de custo a cada 10.000 interações.\n",
    "    lambd -- hiper parâmetro de regularização, valor escalar.\n",
    "    keep_prob - probabilidade de se manter um neurônio durante a execução do dropout, valor escalar.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- os parâmetros aprendidos pelo modelo. Ele pode ser utilizado para prever novas saídas.\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []                            # armazena os valores do custo\n",
    "    m = X.shape[1]                        # número de exemplos\n",
    "    layers_dims = [X.shape[0], 20, 3, 1]\n",
    "    \n",
    "    # Inicializa o dicionário de parâmetros.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradiente descendente)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Propagação para frente: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a3, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Função de custo\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a3, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Propagação para trás.\n",
    "        assert(lambd==0 or keep_prob==1)    # é possível utilizar tanto a regularização L2 como o dropout, \n",
    "                                            # mas nesta tarefa iremos explorar um de cada vez.\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Atualiza Parâmetros.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Imprime a perda a cada 10.000 interações\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Custo após a interação {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('custo')\n",
    "    plt.xlabel('interações (x 1.000)')\n",
    "    plt.title(\"Taxa de aprendizado =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos treinar o modelo sem o uso de regularização e observar a acurácia nos conjuntos de treinamento e de teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y)\n",
    "print (\"No conjunto de treinamento:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"No conjunto de teste:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não se preocupe com o erro de aproximação apresentado. Note que a acurácia no conjunto de treinamento é de 87,5% enquanto que a acurácia no conjunto de teste é de 84%. Este éo nosso **modelo básico**. Vamos ver o efeito do uso de regularização neste modelo. Execute a célula abaixo para plotar a linha de decisão encontrada por este modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Model without regularization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo sem regularização parece estar super ajustado aos dados de treinamento. Ele está se ajustando a pontos com ruído. Vamos ver o que acontece quando utilizamos reguralização para reduzir super ajuste \"overfitting\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Regularização L2\n",
    "\n",
    "A forma padrão de se evitar o super ajuste é chamada de **Regularização L2**. ela consiste em modificar de forma apropriada a função de custo, do: \n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "Para:\n",
    "$$J_{regularizado} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{custo de entropia cruzada} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{Custo da regularização L2} \\tag{2}$$\n",
    "\n",
    "Vamos modificar o custo e observar as consequencias.\n",
    "\n",
    "**Exercício**: Implemente `compute_cost_with_regularization()` que computa o custo dado pela fórmula (2). Para calcular $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Note que você deve fazer isto para $W^{[1]}$, $W^{[2]}$ e $W^{[3]}$, então some os três termos e multiplique por $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: compute_cost_with_regularization\n",
    "\n",
    "def compute_cost_with_regularization(A3, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implemente a função de custocom regularização L2. Veja a fórmula (2) acima.\n",
    "    \n",
    "    Argumentos:\n",
    "    A3 -- pós-ativação, saída da propagação para frente, no formato (tamanho da saída, número de exemplos)\n",
    "    Y -- vetor com saídas corretas, no formato (tamanho da saída, número de exemplos)\n",
    "    parameters -- dicionário python contendo os parâmetros do modelo.\n",
    "    \n",
    "    Retorna:\n",
    "    cost - valor da função de custo regularizada (fórmula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    \n",
    "    cross_entropy_cost = compute_cost(A3, Y) # Isto dá a você a parte da entropia cruzada do custo.\n",
    "    \n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (aprox. 1 linha)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3, Y_assess, parameters = compute_cost_with_regularization_test_case()\n",
    "\n",
    "print(\"custo = \" + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **cost**\n",
    "    </td>\n",
    "        <td>\n",
    "    1.78648594516\n",
    "    </td>\n",
    "    \n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que, como você alteru a função de custo, você deve também modificar a propagação para trás! Todos os gradientes devem ser computados com relação a este novo custo. \n",
    "\n",
    "**Exercício**: Implemente as mudanças necessárias na propagação para trás de forma a considerar o efeito de regularização. As mudanças dizem respeito apenas a dW1, dW2 e dW3. Para cada um, você deve adicionar o termo do gradiente da regularização  ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: backward_propagation_with_regularization\n",
    "\n",
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implementa a propagação para trás do modelo básicoonde foi adicionada regularização L2.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- dados de entrada, no formato (tamanho da entrada, número de exemplos)\n",
    "    Y -- vetor com valores corretos de saída, no formato (tamanho da saída, número de exemplos)\n",
    "    cache -- cache com a saída da propagação para frente\n",
    "    lambd -- hiper parâmetro de regularização, valor escalar\n",
    "    \n",
    "    Retorna:\n",
    "    gradients -- Um dicionário com os gradientes com relação a cada parâmetro, variáveis de ativação e pré-ativação\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    \n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (aprox. 1 linha)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (aprox. 1 linha)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (aprox. 1 linha)\n",
    "    \n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()\n",
    "\n",
    "grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = 0.7)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"dW2 = \"+ str(grads[\"dW2\"]))\n",
    "print (\"dW3 = \"+ str(grads[\"dW3\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **dW1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.25604646  0.12298827 -0.28297129]\n",
    " [-0.17706303  0.34536094 -0.4410571 ]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **dW2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.79276486  0.85133918]\n",
    " [-0.0957219  -0.01720463]\n",
    " [-0.13100772 -0.03750433]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **dW3**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-1.77691347 -0.11832879 -0.09397446]]\n",
    "    </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora executar o modelo utilizando a regularização L2 $(\\lambda = 0.7)$. A função `model()` irá chamar: \n",
    "- `compute_cost_with_regularization` no lugar de `compute_cost`\n",
    "- `backward_propagation_with_regularization` no lugar de `backward_propagation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, lambd = 0.7)\n",
    "print (\"No conjunto de treinamento:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"No conjunto de teste:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As acurácias não variaram muito embora tenham diminuído um pouco, porém, não deve ter ocorrido um super ajuste aos dados de treinamento. Vamos plotar  o linha de decisão do dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Modelo com Regularização L2\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Observações**:\n",
    "- O valor de $\\lambda$ é um hiper parâmetro que você pode ajustar utilizando um conjunto de desenvolvimento. Se $\\lambda$ é muito grande, é possível obter um modelo com bias alto.\n",
    "- A regularização L2 determina uma lnha clara de separação dos dados o que deve facilitar a vida do goleiro da França, se compararmos com o que foi dado pelo modelo básico.  \n",
    "\n",
    "**O que a regularização L2 está realmente fazendo?**:\n",
    "\n",
    "A regularização L2 assume que o modelo com pesos pequenos é mais simples que o modelo com pesos altos. Portanto, por penalizar o quadrado dos valores dos pesos na função de custo faz com que os pesos tenham valores pequenos. O custo compesos altos fica alto também. Isto faz com que o modelo seja mais suave e as saídas mudam mais lentamente.   \n",
    "\n",
    "<font color='blue'>\n",
    "**O que você deve lembrar** -- A implicação da regularização L2:\n",
    "- A computação do custo:\n",
    "    - Um termo de regularização é adicionado ao custo.\n",
    "- A função de propagação para trás:\n",
    "    - Existem termos extras nos gradientes com relação as matrizes de peso.\n",
    "- Pesos acabam ficando menores (\"weight decay\"): \n",
    "    - Os pesos são encaminhados para valores menores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Dropout\n",
    "\n",
    "Finalmente, **dropout** é muito utilizado como técnica de regularização específica de aprendizado profundo. \n",
    "**Ela aleatoriamente desliga alguns neurônios em cada interação.** Assita a estes dois videos para ver o que isto quer dizer!\n",
    "\n",
    "<!--\n",
    "To understand drop-out, consider this conversation with a friend:\n",
    "- Friend: \"Why do you need all these neurons to train your network and classify images?\". \n",
    "- You: \"Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more features my model learns!\"\n",
    "- Friend: \"I see, but are you sure that your neurons are learning different features and not all the same features?\"\n",
    "- You: \"Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution.\"\n",
    "!--> \n",
    "\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "<br>\n",
    "<caption><center> <u> Figura 2 </u>: Dropout na segunda camada escondida. <br> Em cada interação você desliga (= define como zero) cada neurônio de uma camada com probabilidade $1 - keep\\_prob$ ou mantém o neurônio com probabilidade $keep\\_prob$ (50% aqui). Os neurônios desligados não contribuem para o treinamento tanto na propagação para frente como na propagação para trás da interação considerada. </center></caption>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center> <u> Figura 3 </u>: Dropout da primeira e da terceira camada escondida. <br> $1^{a}$ camada: nós desligamos na média 40% dos neurônios.  $3^{a}$ camada: nós desligamos na média 20% dos neurônios. </center></caption>\n",
    "\n",
    "\n",
    "Quando você desliga alguns neurônios, você modifica o seu modelo. A idéia por trás de dropout é que em cada interação você treina um modelo diferente que utiliza apenas um sub conjunto dos neurônios do modelo. Com dropout os neurônios se tornam menos sensitivos para a ativação de um outro neurônio específico, porque aquele neurônio pode ser desligado a qualquer instante.  \n",
    "\n",
    "### 3.1 - Propagação para frente com dropout\n",
    "\n",
    "**Exercício**: Implemente a propagação para frente com dropout. Você está utilizando uma rede com 3 camadas e irá adicionar dropout apenas para a primeira e a segunda camadas escondidas. Não será utilizado o dropout para as camadas de entrada e de saída.    \n",
    "\n",
    "**Instruções**:\n",
    "Você deseja desligar alguns neurônios na primeira e segunda camadas escondidas. Você deve executar as 4 seguintes etapas:\n",
    "1. Nós falamos em criar uma variável $d^{[1]}$ com o mesmo formato de $a^{[1]}$ usando `np.random.rand()` para aleatoriamente gerar números entre 0 e 1. Aqui, você irá utilizar uma implementação vetorizada, portanto, crie uma matriz aleatória $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ da mesma dimensão que $A^{[1]}$.\n",
    "2. Defina cada entrada de $D^{[1]}$ para 0 com probabilidade (`1-keep_prob`) ou 1 com probabilidade (`keep_prob`), fazendo um corte nos valores de $D^{[1]}$ apropriadamente. Dica: para definir todas as entradas de uma matriz X para 0 (se a entrada é menor que 0,5) ou 1 (se a entrada é maior que 0,5) você pode fazer: `X = (X < 0.5)`. Note que 0 e 1 são respectivamente equivalentes a Falso e Verdadeiro.\n",
    "3. Ajuste $A^{[1]}$ para $A^{[1]} * D^{[1]}$. (Você está desligando alguns neurônios). Você pode considerar $D^{[1]}$ como uma máscara, de forma que quando ela é multiplicada por outra matriz, ela desliga alguns de seus valores.\n",
    "4. Divida $A^{[1]}$ por `keep_prob`. Fazendo isto você está assegurando que o rsultado do custo continuará com o mesmo valor esperado se não houvesse o dropout. (Esta técnica é conhecida como dropout invertido). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: forward_propagation_with_dropout\n",
    "\n",
    "def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n",
    "    \"\"\"\n",
    "    Implementa a propagação para frente: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- dados de entrada, no formato (2, número de exemplos)\n",
    "    parameters -- dicionário python contendo os parâmetros \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- matriz de pesos no formato (20, 2)\n",
    "                    b1 -- vetor bias no formato (20, 1)\n",
    "                    W2 -- matriz de pesos no formato (3, 20)\n",
    "                    b2 -- vetor bias no formato (3, 1)\n",
    "                    W3 -- matriz de pesos no formato (1, 3)\n",
    "                    b3 -- vetor bias no formato (1, 1)\n",
    "    keep_prob - probabilidade de manter o neurônioativo durante o dropout, um valor escalar\n",
    "    \n",
    "    Retorna:\n",
    "    A3 -- último valor de ativação, saída da propagação para frente, no formato (1,1)\n",
    "    cache -- tuple, informação armazenada para computação da propagação para trás.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # recupera os parâmetros\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (aprox. 4 linhas)       # Steps 1-4 abaixo correspondem as etapas descritas acima. \n",
    "        # Step 1: initializa a matriz D1 = np.random.rand(..., ...)\n",
    "        # Step 2: converte as entradas de D1 para 0 ou 1 (usando keep_prob como valor de corte)\n",
    "        # Step 3: desliga alguns neurônios de A1\n",
    "        # Step 4: escala o valor dos neurônios que não foram desligados\n",
    "    ### TERMINA O CÓDIGO AQUI ###\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (aprox. 4 linhas)\n",
    "     # Step 1: inicializa a matriz D2 = np.random.rand(..., ...)\n",
    "     # Step 2: converte as entradas de D2 para 0 ou 1 (usando keep_prob como valor de corte)\n",
    "     # Step 3: desliga alguns neurônios de A2\n",
    "     # Step 4: escala o valor dos neurônios que não foram desligados\n",
    "    ### TERMINA O CÓDIGO AQUI ###\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, parameters = forward_propagation_with_dropout_test_case()\n",
    "\n",
    "A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = 0.7)\n",
    "print (\"A3 = \" + str(A3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **A3**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.36974721  0.00305176  0.04565099  0.49683389  0.36974721]]\n",
    "    </td>\n",
    "    \n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Propagação para trás com dropout\n",
    "\n",
    "**Exercício**: Implemene a propagação para trás com dropout. Como antes, você está treinando uma rede com 3 camadas. Adicione dropout para a primeira e segunda camadas escondidas utilizando as máscaras $D^{[1]}$ e $D^{[2]}$ armazenadas na cache. \n",
    "\n",
    "**Instruções**:\n",
    "Propagação para trás com dropout é bem simples. Você deve seguir as 2 etapas abaixo: \n",
    "1. Você desligou previamente alguns neurônios durante a propagação para frente aplicando a máscara $D^{[1]}$ para `A1`. Na propagação para trás você deve desligar os mesmos neurônios utilizando a mesma máscara $D^{[1]}$ para `dA1`. \n",
    "2. Durante a propagação para frente voc6e dividiu `A1` por `keep_prob`. Na propagação para trás, você deverá também dividir`dA1` por `keep_prob` (a interpretação do cálculo é que se $A^{[1]}$ é escalonado por `keep_prob`, então a sua derivada $dA^{[1]}$ também deve ser escalonada pelo mesmo `keep_prob`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: backward_propagation_with_dropout\n",
    "\n",
    "def backward_propagation_with_dropout(X, Y, cache, keep_prob):\n",
    "    \"\"\"\n",
    "    Implementa a propagação para trás do modelo básico quando adicionado dropout.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- dados de entrada, no formato (2, número de exemplos)\n",
    "    Y -- vetor de saída com os valores corretos, no formato (tamanho de saída, número de exemplos)\n",
    "    cache -- cache de saída da função forward_propagation_with_dropout()\n",
    "    keep_prob - probabilidade de manter um neurônio ativo durante o dropout, valor escalar.\n",
    "    \n",
    "    Retorna:\n",
    "    gradients -- Um dicionário com os gradientes relacionados a cada parâmetro, variáveis de ativação e pré-ativação.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "           # Step 1: Aplique a máscara D2 para desligar os mesmos neurônios da propagação para frente\n",
    "           # Step 2: escale o valor dos neurônios que não foram desligados\n",
    "    ### TERMINE O CÓDIGO AQUI ###\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "           # Step 1: Aplique a máscara D1 para desligar os mesmos neurônios da propagação para frente\n",
    "           # Step 2: escale o valor dos neurônios que não foram desligados\n",
    "    ### TERMINE O CÓDIGO AQUI ###\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()\n",
    "\n",
    "gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = 0.8)\n",
    "\n",
    "print (\"dA1 = \" + str(gradients[\"dA1\"]))\n",
    "print (\"dA2 = \" + str(gradients[\"dA2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **dA1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]\n",
    " [ 0.65515713  0.         -0.00337459  0.         -0.        ]]\n",
    "    </td>\n",
    "    \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **dA2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]\n",
    " [ 0.          0.53159854 -0.          0.53159854 -0.34089673]\n",
    " [ 0.          0.         -0.00292733  0.         -0.        ]]\n",
    "    </td>\n",
    "    \n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora executar o modelo com dropout (`keep_prob = 0.86`). Isto quer dizer que a cada interação serão desligados neurônios das camadas escondidas 1 e 2 com 14% de probabilidade. A função `model()` irá chamar:\n",
    "- `forward_propagation_with_dropout` no lugar de `forward_propagation`.\n",
    "- `backward_propagation_with_dropout` no lugar de `backward_propagation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n",
    "\n",
    "print (\"No conjunto de treinamento:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"No conjunto de teste:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não se preocupe com o erro de execução, ele ocorre, conforme já falado, devido a aproximações do python.\n",
    "\n",
    "Dropout funcionou bem! A acurácia permaneceu em 82% para o conjunto de teste e 87,5% para o conjunto de treinamento e a área definida no campo não está super ajustada como no caso do modelo básico. O time da França será eternamente grato a você!!! \n",
    "\n",
    "Execute o código abaixo para ver a função limite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Modelo com dropout\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-0.75,0.40])\n",
    "axes.set_ylim([-0.75,0.65])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Nota**:\n",
    "- Um **erro comum** quando utilizamos dropout é utilizá-lo nos conjuntos de treinamento e de teste. O dropout deve ser utilizado apenas no treinamento e desligado no teste.  \n",
    "- Frameworks de aprendizado profundo como [tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout) ou [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) vem com uma implementação de camada de dropout. Não se preocupe - iremos estudar alguns destes frameworks mais a frente.  \n",
    "\n",
    "<font color='blue'>\n",
    "**O que você deve se lembrar sobre dropout:**\n",
    "- Dropout é uma técnica de regularização.\n",
    "- Você deve utilizar dropout apenas no treinamento e nunca no teste.\n",
    "- Dropout deve ser aplicado nas propagações para frente e para trás.\n",
    "- Durante o treinamento, divida a saída de cada camada que utiliza dropout por keep_prob para manter o mesmo valor esperado de ativação. Por exemplo, se keep_prob é 0.5, então, na média, desligue metade dos nós e a saída deve ser escalonada por 0,5 pois somente metade dos nós estão contribuindo para a solução. Dividindo por 0,5 é o equivalente a multiplicar por 2. Logo, a saída agora terá o mesmo valor esperado. Você pode verificar que isto funciona,mesmo que keep_prob tenha outros valores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aqui estão os resltados dos três modelos**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **modelo**\n",
    "        </td>\n",
    "        <td>\n",
    "        **acurácia treinamento**\n",
    "        </td>\n",
    "        <td>\n",
    "        **acurácia teste**\n",
    "        </td>\n",
    "\n",
    "    </tr>\n",
    "        <td>\n",
    "        RN com 3-camadas sem regularização\n",
    "        </td>\n",
    "        <td>\n",
    "        87,5%\n",
    "        </td>\n",
    "        <td>\n",
    "        84%\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        RN com 3-camadas e usando regularização L2\n",
    "        </td>\n",
    "        <td>\n",
    "        86,5%\n",
    "        </td>\n",
    "        <td>\n",
    "        82%\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        RN com 3-camadas usando dropout\n",
    "        </td>\n",
    "        <td>\n",
    "        87,5%\n",
    "        </td>\n",
    "        <td>\n",
    "        82%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que a regularização penaliza o desempenho no conjunto de teste. Isto ocorre porque ela limita a habilidade da rede de super ajustar aos dados de treinamento. Mas ela acaba fornecendo um modelo mais simples e com resultados semelhantes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parabéns, você concluiu esta tarefa!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color='blue'>\n",
    "**O que você deve lembrar desta tarefa**:\n",
    "- Regularização ajuda a evitar o super ajuste.\n",
    "- Regularização irá levar os pesos para valores mais baixos.\n",
    "- Regularização L2 e Dropout são duas técnicas efetivas de regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "SXQaI",
   "launcher_item_id": "UAwhh"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
