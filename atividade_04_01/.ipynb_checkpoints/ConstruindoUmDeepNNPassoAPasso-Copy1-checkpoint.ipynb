{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construindo um Deep Neural Network: passo a passo\n",
    "\n",
    "Bem-vindo a tarefa da semana 4 (partes 1 e 2)! Você já conseguiu treinar uma rede neural de 2 camadas (com uma única camada escondida). Nesta semana você irá construir uma \"deep neural network\" com o número de camadas escondidas que você desejar. \n",
    "\n",
    "- Neste notebook, você irá implementar todas as funções necessárias para construir uma deep neural network. \n",
    "- Na próxima tarefa você irá utilizar todas estas funções para construir uma deep neural network para classificação de imagens. \n",
    "\n",
    "**Após esta tarefa você será capaz de:**\n",
    "- Utilizar nós não-lineares como ReLu para melhorar o modelo.\n",
    "- Construir uma rede neural mais \"profunda\" (com várias camadas escondidas)\n",
    "- Implementar uma classe fácil de se utilizar de redes neurais.\n",
    "\n",
    "**Notação**:\n",
    "- Sobrescrito $[l]$ indica a quantidade associada com a $l^{th}$ camada. \n",
    "    - Exemplo: $a^{[L]}$ é a $L^{th}$ camada de ativação. $W^{[L]}$ e $b^{[L]}$ são os parâmetros da $L^{th}$ camada.\n",
    "- Sobrescrito $(i)$ indica a quantidade associada com o $i^{th}$ exemplo. \n",
    "    - Exemplo: $x^{(i)}$ é o $i^{th}$ exemplo de treinamento.\n",
    "- Subescrito $i$ indica a $i^{th}$ entrada de um vetor.\n",
    "    - Exemplo: $a^{[l]}_i$ indica a $i^{th}$ entrada da $l^{th}$ camada de ativação.\n",
    "\n",
    "Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Pacotes\n",
    "\n",
    "Primeiro iremos importar todos os pacotes que utilizaremos durante esta tarefa.  \n",
    "- [numpy](www.numpy.org) é o pacote principal para computação científica em python. \n",
    "- [matplotlib](http://matplotlib.org) é a biblioteca do python para plotar gráficos. \n",
    "- dnn_utils contém algumas funções necessárias neste notebook.\n",
    "- testCases contém alguns exemplos que serão utilizados para testar o código deste notebook. \n",
    "- np.random.seed(1) é utilizado para manter a chamada às funções deste notebook consistentes. Não modifique a semente.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - Descrição da tarefa\n",
    "\n",
    "Para construir a rede neural inicialmente iremos implementar algumas funções de apoio. Estas funções serão utilizadas na próxima tarefa para construir uma rede neural com duas camadas escondidas e também para construir uma rede neural com L camadas escondidas. Cada função terá instruções detalhadas para que você consiga implementá-la sem problemas. Abaixo damos uma descrição dos passos desta tarefa.   \n",
    "\n",
    "- Inicialize os parâmetros pra uma rede neural de 2 camadas e para uma rede neural de $L$ camadas.\n",
    "- Implemente a propagação para frente (mostrado em roxo na figura abaixo).\n",
    "     - Complete a parte LINEAR para a etapa de propagação para frente (obtendo assim $Z^{[l]}$).\n",
    "     - É fornecida a função de ativação (relu/sigmoid).\n",
    "     - Combine as duas etapas anteriores em uma nova função para frente [LINEAR->ACTIVATION].\n",
    "     - Empilhe a função para frente [LINEAR->RELU] L-1 vezes (para as camadas de 1 até L-1) e adicione a [LINEAR->SIGMOID] no final (para a camada final $L$). Isto irá criar um modelo novo: modelo_para_frente_L.\n",
    "- Compute a perda.\n",
    "- Implemente a propagação para trás (mostrado em vermelho na figura abaixo).\n",
    "    - Complete a parte LINEAR da etapa de propagação para trás.\n",
    "    - É fornecida a a função gradiente da função ACTIVATE function (relu_backward/sigmoid_backward) \n",
    "    - Combine as duas etapas anteriores em uma nova função para trás [LINEAR->ACTIVATION].\n",
    "    - Empilhe [LINEAR->RELU] para trás L-1 vezes e adicione [LINEAR->SIGMOID] em uma nova função modelo_para_tras_L. \n",
    "- Finalmente atualize os parâmetros.\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center> **Figure 1**</center></caption><br>\n",
    "\n",
    "\n",
    "**Nota** para cada função para frente existe uma função correspondente para trás. Por isso que em cada etapa no módulo para frente  você deverá armazenar alguns valores na cache. Os valores da cache  serão utilizados para computar os gradientes. No módulo de propagação para trás você irá utilizar os valores da cache para determinar os gradientes. Esta tarefa irá te mostrar como executar cada uma destas etapas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Inicialização\n",
    "\n",
    "Aqui você deverá implementar duas funções para inicialização do modelo. A primeira função será utilizada para incializar os parâmetros em uma rede neural com duas camadas escondidas. A segunda função irá generalizar o processo de inicialização para $L$ camadas escondidas. \n",
    "\n",
    "### 3.1 - Rede Neural com 2 camadas escondidas\n",
    "\n",
    "**Exercício**: Crie e inicialize os parâmetros de uma rede neural com duas camadas escondidas.\n",
    "\n",
    "**Instruções**:\n",
    "- A estrutura do modelo é dada por: *LINEAR -> RELU -> LINEAR -> SIGMOID*. \n",
    "- Use uma inicialização aleatória para as matrizes de pesos. Use `np.random.randn(formato)*0.01` com o formato correspondente. \n",
    "- Use uma inicialização com zero para os bias. Use `np.zeros(formato)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: inicializar_parametros\n",
    "\n",
    "def inicializar_parametros(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    n_x -- tamanho da camada de entrada (número de nós)\n",
    "    n_h -- tamanho da camada escondida (número de nós)\n",
    "    n_y -- tamanho da camada de saída (número de nós) \n",
    "    \n",
    "    Retorno:\n",
    "    parametros -- dicionário python contendo os parâmetros:\n",
    "                    W1 -- matriz de pesos no formato (n_h, n_x)\n",
    "                    b1 -- vetor bias no formato (n_h, 1)\n",
    "                    W2 -- matriz de pesos no formato (n_y, n_h)\n",
    "                    b2 -- vetor bias no formato (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parametros = inicializar_parametros(3,2,1)\n",
    "print(\"W1 = \" + str(parametros[\"W1\"]))\n",
    "print(\"b1 = \" + str(parametros[\"b1\"]))\n",
    "print(\"W2 = \" + str(parametros[\"W2\"]))\n",
    "print(\"b2 = \" + str(parametros[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756 -0.00528172]\n",
    " [-0.01072969  0.00865408 -0.02301539]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.01744812 -0.00761207]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 - Rede Neural com L camadas escondidas\n",
    "\n",
    "A inicialização para uma rede profunda (com L camadas escondidas) é mais complicada porque existem mais matrizes de pesos e vetores de bias. Quando completar a função `inicializar_parametros_deep`, você deve ter certeza que as dimensões estão corretas entre cada camada. Lembrando que $n^{[l]}$ é o número de unidades na camada $l$. Por exemplo, se o tamanho da camada de entrada  $X$ é $(12288, 209)$ (com $m=209$ exemplos) então:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Formato de W** </td> \n",
    "        <td> **Formato de b**  </td> \n",
    "        <td> **Ativação** </td>\n",
    "        <td> **Formato da Ativação** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Camada 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Camada 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Camada L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Camada L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Lembre-se que quando computamos $W X + b$ em python, é utilizado broadcasting. Por exemplo, se: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{1}$$\n",
    "\n",
    "Então $WX + b$ será:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{2}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Implemente a função de inicialização para Rede Neural com L camadas. \n",
    "\n",
    "**Instruções**:\n",
    "- A estrutura do modelo é *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. Portanto, ela possui $L-1$ camadas usando a função de ativação ReLU seguido por uma camada de saída com a função de ativação sigmoid. \n",
    "- Use a inicialização aleatória para a matriz de pesos. Utilize `np.random.rand(shape) * 0.01`.\n",
    "- Use a inicialização com zeros para os vetores de bias. Utilize `np.zeros(shape)`.\n",
    "- Vamos armazenar $n^{[l]}$, o número de nós em camadas diferentes, na variável `layer_dims`. Por exemplo, a `layer_dims` para o modelo \"Classificação de Dados Planares\" da última tarefa possuia [2,4,1]: Existiam 2 entradas, uma camada escondida com 4 nós, e um único nó na camada de saída. Isto significa que `W1` tem o formato (4,2), `b1` tem o formato (4,1), `W2` tem o formato (1,4) e `b2` tem o formato (1,1). Agora temos que generalizar isto para $L$ camadas! \n",
    "- Como exemplo, considere que $L=1$ (uma rede neural com uma única camada escondida). isto deve inspirá-lo a implementar um caso geral (Rede Neural com L-camadas escondidas).\n",
    "```python\n",
    "    if L == 1:\n",
    "        parametros[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parametros[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: inicializar_parametros_deep\n",
    "\n",
    "def inicializar_parametros_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- array python (lista) contendo as dimensões de cada camada da rede neural\n",
    "    \n",
    "    Retorna:\n",
    "    parametros -- dicionário python contendo os parâmetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- matriz de pesos no formato (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- vetor de bias no formato (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parametros = {}\n",
    "    L = len(layer_dims)            # número de camadas na rede neural\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "        parametros['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parametros['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "        \n",
    "        assert(parametros['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parametros['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = inicializar_parametros_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Módulo de propagação para frente\n",
    "\n",
    "### 4.1 - Para frente - Linear\n",
    "Agora que os parâmetros foram inicializados podemos implementa o módulo de propagação para frente. Implemente algumas funções básicas que serão utilizadas quando o modelo for implementado. Serão implementadas 3 funções, nesta ordem: \n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION onde ACTIVATION será ou a ReLU ou a Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID (em todo o modelo)\n",
    "\n",
    "O modulo de propagação linear para frente (vetorização sobre todos os exemplos) executa as seguintes equações:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{3}$$\n",
    "\n",
    "onde $A^{[0]} = X$. \n",
    "\n",
    "**Exercício**: Construa a parte linear da propagação para frente.\n",
    "\n",
    "**Lembre-se**:\n",
    "A representação matemática desta unidade é $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$. A função `np.dot()` pode ser útil. Se as dimensões não baterem, imprima `W.shape` para verificar se as dimensões estão corretas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: para_frente_linear\n",
    "\n",
    "def para_frente_linear(A, W, b):\n",
    "    \"\"\"\n",
    "    Implemente a parte linear da propagação para frente.\n",
    "\n",
    "    Argumentos:\n",
    "    A -- ativações da camada anterior (ou dados de entrada): (tamanho da camada anterior, número de exemplos)\n",
    "    W -- matriz de pesos: um array numpy array no formato (tamanho da camada atual, tamanho da camada anterior)\n",
    "    b -- vetor bias, array numpy no formato (tamanho da camada corrente, 1)\n",
    "\n",
    "    Retorna:\n",
    "    Z -- a entrada da função de ativação, também chamada de parâmetro de pré ativação.  \n",
    "    cache -- um dicionário python contendo \"A\", \"W\" e \"b\" ; armazenados para computar a propagação para trás eficientemente.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 1 linha de código)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = para_frente_linear(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Para frente Linear-Ativação\n",
    "\n",
    "Neste serão utilizadas duas funções de ativação:\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. A função `sigmoid` foi fornecida. Esta função retorna **dois** itens: o valor de ativação \"`a`\" e a \"`cache`\" que contém \"`Z`\" (o valor utilizado na propagação para trás correspondente). Para utilizar a função sigmoid faça a seguinte chamada: \n",
    "``` python\n",
    "A, cache_de_ativacao = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: A expressão matemática da ReLu é $A = RELU(Z) = max(0, Z)$. A função `relu` também é fornecida. Esta função também retorna **dois** itens: o valor de ativação \"`A`\" e a \"`cache`\" que contém \"`Z`\" (o valor utilizado na propagação para trás correspondente). Para utilizar a função ReLu faça a seguinte chamada:\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma maior conveniência iremos agrupar duas funções (Linear e Ativação) em apenas uma função (LINEAR->ATIVAÇÃO). Portanto, você implementará a função que executa a etapa LINEAR para frente seguida da etapa ATIVAÇÃO para frente.\n",
    "\n",
    "**Exercício**: Implemente a propagação para frente da camada *LINEAR->ATIVAÇÃO*. A relação matemática é: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ onde a ativação \"g\" pode ser sigmoid() ou relu(). Use a para_frente_linear() e a função de ativação correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: para_frente_linear_ativacao\n",
    "\n",
    "def para_frente_linear_ativacao(A_prev, W, b, ativacao):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para frente para a camada LINEAR->ATIVAÇÃO\n",
    "\n",
    "    Argumentos:\n",
    "    A_prev -- ativações da camada anterior (ou dados de entrada): (tamanho da camada anterior, numero de exemplos)\n",
    "    W -- matriz de pesos: array numpy no formato (tamanho da camada atual, tamanho da camada anterior)\n",
    "    b -- vetor de bias, array numpy no formato (tamanho da camada atual, 1)\n",
    "    ativacao -- the ativacao para ser utilizada nesta camada, armazenada como testo: \"sigmoid\" ou \"relu\"\n",
    "\n",
    "    Retorna:\n",
    "    A -- a saída da função de ativação, também chamado de valor de pós ativação. \n",
    "    cache -- um dicionário python contendo \"cache_linear\" e \"cache_ativacao\";\n",
    "             armazenado para computar a propagação para trás de forma eficiente.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ativacao == \"sigmoid\":\n",
    "        # Entrada: \"A_prev, W, b\". Saida: \"A, activation_cache\".\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = para_frente_linear(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    elif ativacao == \"relu\":\n",
    "        # Entrada: \"A_prev, W, b\". Saída: \"A, activation_cache\".\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = para_frente_linear(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Com sigmoid: A = [[ 0.96890023  0.11013289]]\n",
      "Com ReLU: A = [[ 3.43896131  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = para_frente_linear_ativacao(A_prev, W, b, ativacao = \"sigmoid\")\n",
    "print(\"Com sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = para_frente_linear_ativacao(A_prev, W, b, ativacao = \"relu\")\n",
    "print(\"Com ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **Com sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Com ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Em deep learning, a computação da camada \"[LINEAR->ATIVAÇÃO]\" é contada como uma camada única e não duas camadas da rede neural.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Modelo L-camadas \n",
    "\n",
    "Para uma maior conveniência, quando implementarmos a Rede Neural com $L$ camadas, será necessário uma função que replique a função anterior  (`para_frente_linear_ativacao` com RELU) $L-1$ vezes, seguido de uma chamada para `para_frente_linear_ativacao` com SIGMOID.\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<caption><center> **Figura 2** : modelo *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* </center></caption><br>\n",
    "\n",
    "**Exercício**: Implemente a propagação para frente do modelo acima.\n",
    "\n",
    "**Instrução**: No código abaixo, a variável `AL` representará $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (Isto, as vezes, é chamado também de `Yhat`, isto é $\\hat{Y}$.) \n",
    "\n",
    "**Dicas**:\n",
    "- Use as funções que você já implementou \n",
    "- Use um for loop para replicar [LINEAR->RELU] (L-1) vezes\n",
    "- Não se esqueça de armazenar os resultados na lista \"caches\". Para adicionar um novo valor `c` na `lista`, utilize `list.append(c)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: modelo_para_frente_L\n",
    "\n",
    "def modelo_para_frente_L(X, parameters):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para frente da computação [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID \n",
    "    \n",
    "    Argumentos:\n",
    "    X -- dados, um array numpy array no formato (tamanho da entrada, numero de exemplos)\n",
    "    parameters -- saída da função inicializar_parametros_deep()\n",
    "    \n",
    "    Retorna:\n",
    "    AL -- último valor de pós ativação\n",
    "    caches -- lista das caches contendo:\n",
    "                cada uma das cache da função para_frente_linear_relu() (existem L-1 delas, indexadas de 0 até L-2)\n",
    "                a cache da para_frente_linear_sigmoid() (existe apenas uma, indexada L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # numero de camadas na rede neural\n",
    "    \n",
    "    # Implemente [LINEAR -> RELU]*(L-1). Adicione a \"cache\" para a lista \"caches\".\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "        A, cache = para_frente_linear_ativacao(A_prev, parameters.get(\"W\"+str(l)), parameters.get(\"b\"+str(l)),ativacao = \"relu\")\n",
    "        caches.insert(l, cache)\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    # Implemente LINEAR -> SIGMOID. Adicione a \"cache\" para a lista \"caches\".\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 lines of code)\n",
    "    AL, cache = para_frente_linear_ativacao(A, parameters.get(\"W\"+str(L)), parameters.get(\"b\"+str(L)), ativacao = \"sigmoid\")\n",
    "    caches.insert(L-1,cache)\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[ 0.03921668  0.70498921  0.19734387  0.04728177]]\n",
      "Tamanho da lista caches = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = modelo_para_frente_L(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Tamanho da lista caches = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Tamanho da lista caches ** </td>\n",
    "    <td > 3 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muito bem! Agora você tem uma propagação para frente completa que recebe como entrada X e retorna como saída um vetor linha $A^{[L]}$ contendo as previsões. Ela também armazena os valores intermediários em \"caches\". Usando $A^{[L]}$, você pode computar o custo das previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Função Custo\n",
    "\n",
    "Vamos implementar a propagação para frente e para trás. É necessário determinar o custo, porque é preciso verificar se o modelo está aprendendo.\n",
    "\n",
    "**Exercício**: Compute o custo de entropia cruzada $J$, utilizando a seguinte fórmula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{4}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: compute_custo\n",
    "\n",
    "def compute_custo(AL, Y):\n",
    "    \"\"\"\n",
    "    Implemente a função de custo definida pela equação (4).\n",
    "\n",
    "    Argumentos:\n",
    "    AL -- vetor de probabilidade correspondente as previsões, formato (1, número de exemplos)\n",
    "    Y -- vetor com a classificação correta (por exemplo: 0 se for não-gato, 1 se for gato), formato (1, numero de exemplos)\n",
    "\n",
    "    Retorna:\n",
    "    custo -- custo por entropia cruzada\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute a perda entre aL e y.\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 1 linha de código)\n",
    "    cost = (-1/m)*np.sum(np.multiply(np.log(AL),Y)+np.multiply(np.log(1-AL),(1-Y)))\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # certifique-se que o formato do custo é aquele esperado (e.g. torna [[17]] em 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custo = 0.414931599615\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"custo = \" + str(compute_custo(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "\n",
    "<table>\n",
    "\n",
    "    <tr>\n",
    "    <td>**custo** </td>\n",
    "    <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Modulo de propagação para trás\n",
    "\n",
    "Como na propagação para frente, iremos implementar funções auxiliares para a propagação para trás. Lembre-se que a propagação para trás é utilizada para calcular o gradiente da função de perda com relação aos parâmetros.  \n",
    "\n",
    "**Lembre-se**: \n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "<caption><center> **Figura 3** : Propagação para frente e para trás para *LINEAR->RELU->LINEAR->SIGMOID* <br> *Os blocos roxos representam a propagação para frente, e os blocos vermelhos representam a propagação para trás.*  </center></caption>\n",
    "\n",
    "<!-- \n",
    "Para aqueles que são especialistas em cálculo (voc6e não precisa ser para resolver esta tarefa), a regra de cadeia de cálculo pode ser utilizada para derivar a perda $\\mathcal{L}$ com relação a $z^{[1]}$ numa rede de 2 camadas da seguinte forma:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "Para calcular o gradiente $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, você pode utilizar a regra de cadeia e fazer $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. Durante a propagação para trás, em cada etapa multiplica-se o gradiente atual pelo gradiente correspondente à camada específica para se obter o gradiente desejado. \n",
    "\n",
    "De forma equivalente, para calcular o gradiente $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, utiliza-se a regra de cadeia e faz-se $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "Por isto se fala aqui em **propagação para trás**.\n",
    "!-->\n",
    "\n",
    "Agora, de forma similar a propagação para frente, você irá construir a propagação para trás em três etapas:\n",
    "- LINEAR para trás\n",
    "- LINEAR -> ATIVAÇÃO para trás onde ATIVAÇÃO determina a derivativa da ativação da ReLu ou da sigmoid\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID para trás (modelo completo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Para trás Linear\n",
    "\n",
    "Para a camada $l$, a parte linear é: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguido por uma ativação).\n",
    "\n",
    "Suponha que já tenha sido calculado a derivativa $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. Você deseja obter $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figura 4** </center></caption>\n",
    "\n",
    "As três saídas $(dW^{[l]}, db^{[l]}, dA^{[l]})$ são computadas usando a entrada $dZ^{[l]}$. Abaixo estão as fórmulas que você precisa:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{5}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{6}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Use as 3 fórmulas acima para implementar para_tras_linear()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: para_tras_linear\n",
    "\n",
    "def para_tras_linear(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implemente a porção linear da propagação para trás em uma única camada (camada l)\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- Gradiente do custo com relação à saída linear (d camada atual l)\n",
    "    cache -- tupla de valores (A_prev, W, b) que vem da propagação para frente na camada atual.\n",
    "\n",
    "    Retorna:\n",
    "    dA_prev -- Gradiente do custo com relação a ativação (da camada anterior l-1), no mesmo formato que A_prev\n",
    "    dW -- Gradiente do custo com relação a W (camada corrente l), mesmo formato que W.\n",
    "    db -- Gradiente do custo com relação a b (camada corrente l), mesmo formato de b.\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 3 linhas de código)\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[ 0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Ajuste de valores de teste\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = para_tras_linear(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**: \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **dW** </td>\n",
    "        <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "    </tr> \n",
    "  \n",
    "    <tr>\n",
    "        <td> **db** </td>\n",
    "        <td> [[ 0.50629448]] </td> \n",
    "    </tr> \n",
    "    \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Para trás Linear-Ativação \n",
    "\n",
    "Agora, iremos criar uma função que junta as duas funções auxiliares: **`para_tras_linear`** e a etapa para trás da ativação **`para_tras_linear_ativacao`**. \n",
    "\n",
    "Para ajudar a implementar `para_tras_linear_ativacao`, são fornecidas duas funções para trás:\n",
    "- **`para_tras_sigmoid`**: Implementa a propagação para trás para nós com função de ativação SIGMOID. Sua chamada é feita da seguinte forma:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`para_tras_relu`**: Implementa a propagação para trás para os nós que utilizam RELU. Sua chamada é feita da seguinte forma: \n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "Se $g(.)$ é a função de ativação, `sigmoid_backward` e `relu_backward` computam $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{8}$$.  \n",
    "\n",
    "**Exercício**: Implemente a propagação para trás para a camada *LINEAR->ATIVAÇÃO*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: para_tras_linear_ativacao\n",
    "\n",
    "def para_tras_linear_ativacao(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para trás para a camada LINEAR->ATIVAÇÃO.\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente de pós-ativação para a camada atual l \n",
    "    cache -- tupla de valores (linear_cache, activation_cache) usados para determinar a propagação para trás de forma eficiente.\n",
    "    activation -- a ativação a ser utilizada nesta camada, armazenada como texto: \"sigmoid\" ou \"relu\"\n",
    "    \n",
    "    Retorna:\n",
    "    dA_prev -- Gradiente do custo com relação a ativação (da camada anterior l-1), mesmo formato que A_prev\n",
    "    dW -- Gradiente do custo com relação a W (camada atual l), mesmo formato que W.\n",
    "    db -- Gradiente de custo com relação a b (camada atual l), mesmo formato que b.\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = para_tras_linear(dZ,linear_cache)\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = para_tras_linear(dZ,linear_cache)\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = para_tras_linear_ativacao(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = para_tras_linear_ativacao(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada para sigmoid:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada com relu:**\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - Modelo para trás L \n",
    "\n",
    "Agora iremos implementar a função de propagação para trás para a rede neural toda. Lembre-se que quando implementamos o `modelo_para_frente_L`, em cada interação a cache era armazenada com os valores  de (X,W,b, e z). Na propagação para trás estes valores serão utilizados para determinar os gradientes. Então, no `modelo_para_tras_L` iremos interagir por todas as camadas escondidas de trás para frente, começando pela camada $L$. Em cada etapa, voc6e utilizará os valores armazenados para a camada $l$ para propagar para trás através da camada $l$. A Figura 5 mostra o passo para trás. \n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figura 5** : Passo para trás  </center></caption>\n",
    "\n",
    "** Inicialização da propagação para trás**:\n",
    "Para propagar para trás através desta rede sabemos que a saída é $A^{[L]} = \\sigma(Z^{[L]})$. Seu código deve computar `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "Para fazer isto utilize a fórmula (derivada utilizando cálculo, o qual você não precisa compreender totalmente):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivativa do custo com relação a AL\n",
    "```\n",
    "\n",
    "Você pode então utilizar o gradiente da pós ativação `dAL` para continuar indo para trás. Como visto na Figura 5, você pode alimentar a função para trás implementada LINEAR->SIGMOID com `dAL` (que utilizará os valores armazenados na cache pelo modelo_para_frente_L). Após isto você deverá utilizar um `for` para interagir através das outras camadas utilizando a função para trás LINEAR->RELU. Você deve armazenar cada dA, dW, e db no dicionário grads. Para fazer isto, use a expressão: \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{9} $$\n",
    "\n",
    "Por exemplo, para $l=3$, $dW^{[l]}$ estaria armazendo em `grads[\"dW3\"]`.\n",
    "\n",
    "**Exercício**: Implemente a propagação para trás para o modelo *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: modelo_para_tras_L\n",
    "\n",
    "def modelo_para_tras_L(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implemente a propagação para trás para o grupo [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID.\n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- vetor de probabilidade, saída da propagação para frente (modelo_para_frente_L())\n",
    "    Y -- vetor de classificação correta (contendo 0 se nao-gato, 1 se gato)\n",
    "    caches -- lista de caches contendo:\n",
    "                cada cache obtida na para_frente_linear_ativacao() com \"relu\" (isto é, caches[l], para l no range(L-1) i.e l = 0...L-2)\n",
    "                a cache da para_frente_linear_ativacao() com \"sigmoid\" (isto é, caches[L-1])\n",
    "    \n",
    "    Retorna:\n",
    "    grads -- Um dicionário com os gradientes\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # o número de camadas\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # após esta linha, Y tem o mesmo formato de AL\n",
    "    \n",
    "    # Inicializando a propagação para trás\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (1 linha de código)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    # L-ésima camada (SIGMOID -> LINEAR), gradientes. Entrada: \"AL, Y, caches\". Saída: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (approx. 2 linhas)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = para_tras_linear_ativacao(dAL,current_cache, 'sigmoid')\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # l-ésima camada: (RELU -> LINEAR), gradientes.\n",
    "        # Entrada: \"grads[\"dA\" + str(l + 2)], caches\". Saída: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (approx. 5 linhas)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = para_tras_linear_ativacao(grads.get(\"dA\"+str(l+2)),current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = modelo_para_tras_L(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.12913162 -0.44014127]\n",
    " [-0.14175655  0.48317296]\n",
    " [ 0.01663708 -0.05670698]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Atualização de Parâmetros\n",
    "\n",
    "Nesta parte da tarefa iremos atualizar os parâmetros do modelo utilizando gradiente descendente:  \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{10}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{11}$$\n",
    "\n",
    "onde $\\alpha$ é a taxa de aprendizado. Após computar os parâmetros atualizados armazene estes parâmetros no dicionário de parâmetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício**: Implemente `atualize_parametros()` para atualizar os parâmetros utilizando gradiente descendente.\n",
    "\n",
    "**Instruções**:\n",
    "Atualize os parâmetros utilizando gradiente descendente para cada $W^{[l]}$ e $b^{[l]}$ for $l = 1, 2, ..., L$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: atualizacao_parametros\n",
    "\n",
    "def atualizacao_parametros(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Atualize os parametros utilizando gradiente descendente.\n",
    "    \n",
    "    Argumentos:\n",
    "    parameters -- dicionario python contendo os parâmetros.  \n",
    "    grads -- dicionário python contendo os gradientes, saída do modelo_para_tras_L.\n",
    "    learning_rate -- taxa de aprendizado\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- dicionario python contendo os parâmetros atualizados. \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # número de camadas na rede neural\n",
    "\n",
    "    # Regra de atualizacao para cada parâmetro. Utilize um for loop.\n",
    "    ### INICIE O SEU CÓDIGO AQUI ### (≈ 3 linhas de código)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)]=parameters.get(\"W\"+str(l+1))-learning_rate*grads.get(\"dW\"+str(l+1))\n",
    "        parameters[\"b\"+str(l+1)]=parameters.get(\"b\"+str(l+1))-learning_rate*grads.get(\"db\"+str(l+1))\n",
    "    ### TÉRMINO DO CÓDIGO ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = atualizacao_parametros(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída Esperada**:\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  \n",
    "    <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - Conclusão\n",
    "\n",
    "Parabéns ao completar esta tarefa. Você implementou todas as funções necessárias para construir uma rede neural profunda! \n",
    "\n",
    "Sabemos que esta era uma tarefa longa porém, olhando para frente, isto apenas vai melhorar. A proxima tarefa deve ser mais fácil. \n",
    "\n",
    "Na próxima tarefa você irá colocar tudo junto e construir dois modelos: \n",
    "- Uma rede neural com duas camadas escondidas.\n",
    "- Uma rede neural com L camadas.\n",
    "\n",
    "Você irá utilizar estes modelos para classificar imagens entre gatos e não-gatos. Divirta-se!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
