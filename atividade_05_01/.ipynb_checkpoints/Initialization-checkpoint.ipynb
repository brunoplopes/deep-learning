{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização\n",
    "\n",
    "Bem-vindo a primeira tarefa da parte 2 do curso de Aprendizado de Máquinas Profundo. \n",
    "\n",
    "Treinar uma rede neural requer a especificação de um valor inicialpara os pesos. Um processo de inicialização mais adequado irá auxiliar no aprendizado.  \n",
    "\n",
    "Nesta tarefa iremos estudar a inicialização de uma rede neural analisando formas diferentes de inicialização e verificar os resultados obtidos.  \n",
    "\n",
    "Uma escolha correta da inicialização deverá:\n",
    "- Acelerar a convergência do gradiente descendente\n",
    "- Aumentam as chances do gradiente descendente convergir para um erro menor no treinamento (e generalização)  \n",
    "\n",
    "Para começar execute a próxima célula e carregue os pacotes necessários e os dados com que iremos trabalhar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
    "from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# carrega a base de dados: pontos azul/vermelho em circulos\n",
    "train_X, train_Y, test_X, test_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você deseja criar um classificador que separe os pontos azuis dos pontos vermelhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Modelo de Rede Neural "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta tarefa iremos utilizar uma rede neural com 3 camadas (já implementada para você). Aqui estão os métodos de inicialização que você irá experimentar:   \n",
    "- *Inicialização com zeros* --  ajustando `initialization = \"zeros\"` no argumento de entrada.\n",
    "- *Inicialização aleatória* -- ajustando `initialization = \"random\"` no argumento de entrada. Este método inicializa os pesos com valores aleatórios, porém, grandes.  \n",
    "- *Inicialização \"he\"* -- ajustando `initialization = \"he\"` no argumento de entrada. Este método inicializa os pesos para valores aleatórios de acordo com o artigo He et al., 2015. \n",
    "\n",
    "**Instruções**: Por favor leia rapidamente o código abaixo e execute a célula. Nas próximas células você irá implementar os três processos de inicialização para este modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n",
    "    \"\"\"\n",
    "    Implemente a rede neural em 3 camadas: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- dados de entrada, no formato (2, numero de exemplos)\n",
    "    Y -- vetor com os valores corretos de saída (contendo 0 para pontos vermelhos; 1 para pontos azuis), no formato \n",
    "         (1, número de exemplos)\n",
    "    learning_rate -- taxa de aprendizado do gradiente descendente. \n",
    "    num_iterations -- número de interações do gradiente descendente\n",
    "    print_cost -- se True, imprime o custo a cada 1000 interações\n",
    "    initialization -- flag para escolha do método de inicialização \n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- parâmetros aprendidos pelo modelo\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = [] # para manter os valores do custo\n",
    "    m = X.shape[1] # número de exemplos\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "    \n",
    "    # Inicializa os parâmetros do dicionário.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradiente descendente)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Propagação para frente: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Custo\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Propagação para trás.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Atualização de parâmetros\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Imprime o custo a cada 1000 interações\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Custo após a interação {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plota o custo\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('custo')\n",
    "    plt.xlabel('iterações (por centenas)')\n",
    "    plt.title(\"Taxa de aprendizado =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Inicialização com Zeros\n",
    "\n",
    "Existem dois tipos de parâmetros a serem inicializados em uma rede neural:\n",
    "- as matrizes de peso $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- os vetores de bias $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "\n",
    "**Exercício**: Implemente a função a seguir para inicializar todos os parâmetros com zeros. Voce verá, depois, que está inicialização não funciona muito bem pois ele falha na hora que \"quebrar simetrias\" mas, vamos tentar usar este processo de qualquer forma e ver o que acontece. Utilize np.zeros((..,..)) com os formatos corretos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: initialize_parameters_zeros \n",
    "\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- array de python (lista) contendo o tamanho de cada camada.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- dicionário python contendo os parâmetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- matriz de pesos no formato (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- vetor de bias no formato (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- matriz de pesos no formato (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- vetor de bias no formato (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # número de camadas na rede\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### INICIE SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "        \n",
    "        \n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.  0.  0.]\n",
    " [ 0.  0.  0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.  0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para treinar seu modelo utilizando 15.000 interações utilizando inicialização com zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"No conjunto de treinamento:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"No conjunto de teste:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O desempenho é bem ruim, e o custo não diminui. O desempenho deste modelo não é muito melhor que uma escolha aleatória. Porque? Vamos analisar os detalhes da predição e a borda de decisão: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"predictions_train = \" + str(predictions_train))\n",
    "print (\"predictions_test = \" + str(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Modelo com inicialização com zeros\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo está fazendo uma previsão de 0 para todos os exemplos. \n",
    "\n",
    "Em geral, inicializar todos os pesos com zero resulta em uma rede que não consegue quebrar simetrias. Isto quer dizer que cada neurônio em cada camada irá aprender exatamente a mesma coisa, e você pode estar treinando uma rede neural com $n^{[l]}=1$ para todas as camadas, e a rede não é mais poderosa que um classificador linear como a regressão logística.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**O que você deve lembrar**:\n",
    "- Os pesos $W^{[l]}$ devem ser inicializados aleatoriamente para quebrar a simetria. \n",
    "- Não existe problema em se inicializar os bias $b^{[l]}$ com zeros. A simetria ainda é quebrada desde que $W^{[l]}$ seja inicializado aleatoriamente. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Inicialização aleatória\n",
    "\n",
    "Para quebrar a simetria da rede vamos inicializar os pesos aleatoriamente. Seguindo a inicialização aleatória, cada neurônio poderá aprender uma função diferente para suas entradas. Neste exercício, você verá o que acontece se os pesos são inicializados aleatoriamente, porém, com valores altos. \n",
    "\n",
    "**Exercício**: Implemente a seguinte função para inicializar os pesos para valores aleatórios altos (ajustados para \\*10) e os bias com zeros. Utilize `np.random.randn(..,..) * 10` para os pesos e `np.zeros((.., ..))` para os bias. Nós estamos utilizando uma semente fixa `np.random.seed(..)` para garantir os resultados, portanto, não se preocupe em executar o código várias vezes, os valores devem ser sempre os mesmos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE AVALIAÇÃO: initialize_parameters_random\n",
    "\n",
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- um array python (lista) contendo o tamanho de cada camada.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- dicionário python contendo os parâmetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- matriz de pesos no formato (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- vetor de bias no formato (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- matriz de pesos no formato  (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- vetor de bias no formato  (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)               # Esta semente garante que os valores aleatórios serão sempre os mesmos\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # valor inteiro representando o número de camadas\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### INICIE O SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "         \n",
    "            \n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 17.88628473   4.36509851   0.96497468]\n",
    " [-18.63492703  -2.77388203  -3.54758979]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.82741481 -6.27000677]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute o código abaixo para treinar o seu modelo com 15.000 interações utilizando a inicialização aleatória. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"random\")\n",
    "print (\"No conjunto de treinamento:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"No conjunto de teste:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você obtiver um \"inf\" como o custo após a interação 0, isto ocorre devido a um problema de arredondamento no python; uma implementação mais sofisticada resolveria este problema, mas isto não afeta o nosso propósito neste exercício.  \n",
    "\n",
    "De qualquer forma, note que a simetria foi quebrada e os resultados obtidos parecem melhores que os anteriores. A saída do modelo não é mais de apenas zeros.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (predictions_train)\n",
    "print (predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Modelo com inicialização por valores aleatórios grandes\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações**:\n",
    "- O custo começa muito alto. Isto ocorre porque os valores aleatórios da inicialização são altos. A última ativação (sigmoid) da como saída valores muito próximos de 0 ou de 1 para alguns exemplos e quando o resultado está errado o valor da perda é grande para aquele exemplo. De fato, quando $\\log(a^{[3]}) = \\log(0)$, a perda vai para infinito.\n",
    "- Uma inicialização ruim pode levar a gradientes que vão para zero ou que explodem fazendo com que o algoritmo de otimização seja lento.  \n",
    "- Se você treinar esta rede por mais tempo você deve conseguir melhores resultados porém, a inicialização com valores aleatórios grandes desacelera o processo de aprendizado. \n",
    "\n",
    "<font color='blue'>\n",
    "**Em Resumo**:\n",
    "- Inicializar os pesos para valores aleatórios grandes não funciona muito bem. \n",
    "- Esperamos que a inicialização com valores aleatórios pequenos se comporte de uma melhor forma. A questão importante é: quão pequeno devem ser estes valores? Vamos tentar descobrir no próximo exercício.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Inicialização He\n",
    "\n",
    "Finalmente, vmos experimentar a inicialização He; este nome foi escolhido por ser o nome do primeiro autor do artigo He et al., 2015. (Se você já ouviu falar da inicialização de Xavier, está é similar, exceto que na Xavier é utilizado um fator de escala para os pesos $W^{[l]}$ de `sqrt(1./layers_dims[l-1])` enquanto que na inicialização He é utilizado `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "**Exercício**: Implemente a seguinte função de inicialização usando o processo de He.\n",
    "\n",
    "**Dica**: Esta função é similar a prévia `initialize_parameters_random(...)`. A única diferença é que ao invés de multiplicar  `np.random.randn(..,..)` por 10, você irá multiplicá-los por $\\sqrt{\\frac{2}{\\text{dimensão da camada anterior}}}$, que é o que é feito na inicialização He para as camadas com função de ativação ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNÇÃO DE INICIALIZAÇÃO: initialize_parameters_he\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- array python (lista) contendo o tamanho de cada camada.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters -- um dicionário python contendo os parâmetros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- matriz de pesos no formato (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- vetor de bias no formato (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- matriz de pesos no formato (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- vetor de bias no formato (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # valor inteiro representando o número de camadas\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        ### INICIE SEU CÓDIGO AQUI ### (≈ 2 linhas de código)\n",
    "       \n",
    "    \n",
    "        ### TÉRMINO DO CÓDIGO ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 1.78862847  0.43650985]\n",
    " [ 0.09649747 -1.8634927 ]\n",
    " [-0.2773882  -0.35475898]\n",
    " [-0.08274148 -0.62700068]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para treinar seu modelo com 15.000 interações utilizando a inicialização He. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"he\")\n",
    "print (\"Sobre o conjunto de treinamento:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"Sobre o conjunto de teste:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observações**:\n",
    "- O modelo com a inicialização He separa ospontos vermelhos dos azuis muito bem com um número pequeno de interações. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 - Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Você viu 3 formas diferentes de inicializar os parâmetros de uma rede neural. Para o mesmonúmero de interações e o mesmo conjunto de hiper-parâmetros, a comparação é a seguinte: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Modelo**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Acurácia do treinamento**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Problema/Comentário**\n",
    "        </td>\n",
    "\n",
    "    </tr>\n",
    "        <td>\n",
    "        Rede de 3-camadas inicializada com zeros.\n",
    "        </td>\n",
    "        <td>\n",
    "        50%\n",
    "        </td>\n",
    "        <td>\n",
    "        falha na quebra de simetria\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Rede de 3-camadas inicializada aleatoriamente.\n",
    "        </td>\n",
    "        <td>\n",
    "        83%\n",
    "        </td>\n",
    "        <td>\n",
    "        pesos muito grandes \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        Rede de 3-camadas inicializada com o método de He\n",
    "        </td>\n",
    "        <td>\n",
    "        99%\n",
    "        </td>\n",
    "        <td>\n",
    "        método recomendado\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**O que você deve lembrar deste notebook**:\n",
    "- Inicializações diferentes levam a resultados diferentes.\n",
    "- Inicialização aleatória é utilizada para quebrar simetrias e fazer com que unidades diferentes aprendam coisas diferentes. \n",
    "- Não inicialize com valores muito grandes.\n",
    "- A inicialização He funciona bem para redes com fnção de ativação ReLU.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "XOESP",
   "launcher_item_id": "8IhFN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
